[
["index.html", "Statistical Modeling: Computational Technique Preface: Updating Computational Technique The story in more detail The story continues …", " Statistical Modeling: Computational Technique Daniel Kaplan and Frank Shaw September 2016 Preface: Updating Computational Technique The story in short: Things change, especially software, and including the mosaic package that the computational technique sections in Statistical Modeling: A Fresh Approach were based on. This document revises those sections to bring them up to date with the version of mosaic currently provided to R users. The story in more detail The first edition of Statistical Modeling: A Fresh Approach came out in 2009. That edition included add-in software for R that provided several custom features that made it easier to work with R for introductory students. Probably the most famous component of that software is the do() function. But there are others, such as rand() and resample(). Soon after the first edition appeared, the US National Science Foundation approved funding for Project MOSAIC, a collaboration between Fresh Approach author Daniel Kaplan (Macalester College) and Randall Pruim (Calvin College), Nicholas Horton (Amherst College), and Eric Marland (Appalachian State University). Three of us — Pruim, Horton, and Kaplan — developed an R package based in large part on the software written for the first edition of Fresh Approach, but extended in important ways. A particularly important extension was the adoption of a user friendly interface for basic statistics that uses the “formula” notation in R. (See the mosaic vignette Less Volume: More Creativity.) The second edition of Statistical Modeling: A Fresh Approach made direct use of the mosaic package. This entailed a complete re-write of the computational technique chapters from the first edition. (Other changes were made as well.) The second edition came out at the end of 2011. Since 2011, the MOSAIC project and its mosaic package have continued to thrive. Our best estimate of the number of students who have made use of mosaic is about 200,000, as of the end of 2015. In addition to being used in A Fresh Approach, there are mosaic companions for several other textbooks: Lock 5 — Statistics: Unlocking the Power of Data link to companion Tintle, et al — Introduction to Statistical Investigation link to companion Moore/McCabe — Introduction to the Practice of Statistics link to companion These companions are the moral equivalent of the computational technique sections for A Fresh Approach, but those books do not have the emphasis on modeling and multiple explanatory variables that is the hallmark of A Fresh Approach. The mosaic package has evolved in the years since 2011. Some of the changes have introduced slight incompatibilities with the contents of the 2nd edition computational technique chapters. This document brings those chapter up to date. Obviously, we can’t update the many printed books already in circulation, so we are offering this update publicly via the web. The story continues … These updates to the 2nd edition computational technique sections are part of a wider revision of A Fresh Approach to integrate it more closely to the data wrangling and visualization techniques that have become more accessible and powerful in the last few years, and adoption of a new publishing platform that will provide electronic access (as well as attractively priced printed access), make use of the vastly expanded capabilities of R, RMarkdown, and other publishing activities, provide a free and open source platform for interactive exercises, and link to online tutoring services such as DataCamp. The third edition of A Fresh Approach will be even fresher. It will streamline the introduction to modeling by using concepts and techniques from machine learning (e.g. cross-validation as a central inference technique). The second edition’s emphasis on model coefficients will be given a secondary role, as a specialized set of techniques but no longer as the primary engine for introducing concepts such as variation, co-variation, effect size, etc. A prototype for the software to be used in the third edition is available at &lt;github.com/dtkaplan/StatisticalModeling&gt;. This will likely undergo considerable revision and extension as the third edition is developed. Two online short courses are already available using the new approach of the third edition. These include short videos presenting concepts and many new computational exercises to develop the student’s understanding of and capacity for statistical modeling. Statistical Modeling: Part I Statistical Modeling: Part II "],
["introduction.html", "Chapter 1 Introduction 1.1 The Choice of Software 1.2 The R Command Console 1.3 Invoking an Operation", " Chapter 1 Introduction Modern statistics is done on the computer. There was a time, 60 years ago and before, when computation could only be done by hand or using balky mechanical calculators. The methods of applied statistics developed during this time reflected what could be done using such calculators, not necessarily what was best for illuminating the system under study. These methods took on a life of their own — they became the operational definition of statistics. They continue to be taught today, using electronic calculators or personal computers or even just using paper and pencil. For the old statistical methods, computers are merely a labor saving device. But not for modern statistics. The statistical methods at the core of this book cannot be applied in a authentic and realistic way without powerful computers. Thirty years ago, many of the methods could not be done at all unless you had access to the resources of a government agency or a large university. But with the revolutionary advances in computer hardware and numerical algorithms over the last half-century, modern statistical calculations can be performed on an ordinary home computer or laptop. (Even a cell phone has phenomenal computational power, often besting the mainframes of thirty years ago.) Hardware and software today pose no limitation; they are readily available. Each chapter of this book includes a section on computational technique. Many readers will be completely new to the use of computers for scientific and statistial work, so the first chapters cover the foundations, techniques that are useful for many different aspects of computation. Working through the early chapters is essential for developing the skills that will be used later in actual statistical work. It will take a few hours, but this investment will pay off handsomely. Chances are, you use a computer almost every day: for email, word-processing, managing your music or your photograph collection, perhaps even using a spreadsheet program for accounting. The software you use for such activities makes it easy to get started. Possibly you have never even looked at an instruction manual or used the “help” features on your computer. When you use a word processor or email, the bulk of what you enter into the computer — the content of your documents and email — is without meaning to the computer. This is not at all to say that it is meaningless. Your documents and letters are intended for human readers; most of the work you do is directed so that the recipients can understand them. But the computer doesn’t need to understand what you write in order to format it, print it, or transmit it over the Internet. When doing scientific and statistical computing, things are different. What you enter into the computer is instructions to the computer to perform calculations and re-arrangements of data. Those instructions have to be comprehensible to the computer. If they make no sense or if they are inconsistent or ill formed, the computer won’t be able to carry out your instructions. Worse, if the instructions make sense in some formal way but don’t convey your actual intentions, the computer will perform some operation but the result will mislead you. The difficulty with using software for mathematics and statistics is in making sure that your instructions make sense and do what you want them to do. This difficulty is not a matter of bad software design; it’s intrinsic to the problem of communicating your intentions to the computer. The same difficulty would arise in word processing if the computer had to make sense of your writing, rejecting it when a claim is unconvincing or when a sentence is ambiguous. Statistical computing pioneer John Chambers refers to the “Prime Directive” of software (???): “to program in such a way that computations can be understood and trusted.” Much of the design of software for scientific and statistical work is oriented around the difficulty of communicating intentions. A popular approach is based on the computer mouse: the program provides a list of possible operations — like the keys on a calculator — and lets the user choose which operation to apply to some selected data. This style of user interface is employed, for example, in spreadsheet software, letting users add up columns of numbers, make graphs, etc. The reason this style is popular is that it can make things extremely easy … so long as the operation that you want has been included in the software. But things get very hard if you need to construct your own operation and it can be difficult to understand or trust the operations performed by others. Another style of scientific computation — the one used in this book — is based on language. Rather than selecting an option with a mouse, you construct a command that conveys both the operation that you want and the data to which you want to apply that operation. There are dramatic advantages to this language-based style of computation: It lets you connect computations to one another, so that the output of one operation can become the input to another. It lets you repeat the operation on new or modified data, allowing you to automate tedious tasks and, importantly, to verify the correctness of your computations on data where you already know the answer. It lets you accumulate the results of previous operations, treating those results as new data. It lets you document concisely what was done so that you can demonstrate that what you said you did is what you actually did. In this way, you or others can repeat the analysis later if necessary to confirm your results. It lets you modify the computation in a controlled way to correct it or to vary some aspect of it while holding other aspects exactly the same as in the original. In order to use the language-based approach, you will need to learn a few principles of the language itself: some vocabulary, some syntax, some grammar. This is much, much easier for the computer language than for a natural language like English or Chinese; it will take you only a couple of hours before you are fluent enough to do useful computations. In addition to letting you perform scientific computations in ways that use the computer and your own time and effort effectively, the principles that you will learn are broadly applicable to many computer systems and can provide significant insight even to how to use mouse-based interfaces. 1.1 The Choice of Software The software package used in this book is called R. The R package provides an environment for doing statistical and scientific computation at a professional level. It was designed for statistics work, but suitable for other forms of scientific calculations and the creation of high-quality scientific graphics. There are several other major software packages widely used in statistics. Among the leaders are SPSS, SAS, and Stata. Each of them provides the computational power needed for statistical modeling. Each has its own advantages and its own devoted group of users. One reason for the choice of R is that it offers a command-based computing environment. That makes it much easier to write about computing and also reveals better the structure of the computing process. (???) R is available for free and works on the major types of computers, e.g., Windows, Macintosh, and Unix/Linux. The RStudio software lets you work with a complete R system using an ordinary web browser or on your own computer. In making your own choice, the most important thing is this: choose something! Readers who are familiar with SPSS, SAS, or STATA can use the information in each chapter’s computational technique section to help them identify the facilities to look for in those packages. Another form of software that’s often used with data is the spreadsheet. Examples are Excel and Google Spreadsheets. Spreadsheets are effective for entering data and have nice facilities for formatting tables. The visual layout of the data seems to be intuitive to many people. Many businesses use spreadsheets and they are widely taught in high schools. Unfortunately, they are very difficult to use for statistical analyses of any sophistication. Indeed, even some very elementary statistical tasks such as making a histogram are difficult with spreadsheets and the results are usually unsatisfactory from a graphical point of view. Worse, spreadsheets can be very hard to use reliably. There are lots of opportunities to make mistakes that will go undetected. As a result, despite the popularity of spreadsheets, I encourage you to reserve them for data entry and consider other software for the analysis of your data. 1.2 The R Command Console Depending on your circumstances, you may prefer to install R as software on your own computer, or use a web-based (“cloud computing”) service such as RStudio that runs through a web browser. If you are using this book with a course, your instructor may have set up a system for you. If you are on your own, follow the set-up instructions available on-line at . Depending on whether you run R on your own computer or in the cloud, you will start R either by clicking on an icon in the familiar way or by logging in to the web service. In either case, you will see a console panel in which you will be typing your commands to R. It will look something like Figure ??.1 Figure 1.1: The R command console. The R console gives you great power to carry out statistical and other mathematical and scientific operations. To use this power, you need to learn a little bit about the syntax and meaning of R commands. Once you have learned this, operations become simple to perform. 1.3 Invoking an Operation People often think of computers as doing things: sending email, playing music, storing files. Your job in using a computer is to tell the computer what to do. There are many different words used to refer to the “what”: a procedure, a task, a function, a routine, and so on. I’ll use the word computation. Admittedly, this is a bit circular, but it is easy to remember: computers perform computations. Complex computations are built up from simpler computations. This may seem obvious, but it is a powerful idea. An algorithm is just a description of a computation in terms of other computations that you already know how to perform. To help distinguish between the computation as a whole and the simpler parts, it is helpful to introduce a new word: an operator (??? software!operator) performs a computation. It’s helpful to think of the computation carried out by an operator as involving four parts: The name of the operator The input arguments The output value Side effects A typical operation takes one or more input arguments and uses the information in these to produce an output value. Along the way, the computer might take some action: display a graph, store a file, make a sound, etc. These actions are called side effects. To tell the computer to perform a computation — call this invoking an operation or giving a command — you need to provide the name and the input arguments in a specific format. The computer then returns the output value. For example, the command sqrt(25) invokes the square root operator (named sqrt) on the argument 25. The output from the computation will, of course, be 5. The syntax for invoking an operation consists of the operator’s name, followed by round parentheses. The input arguments go inside the parentheses. The software program that you use to invoke operators is called an interpreter. (The interpreter is the program you are running when you start R.) You enter your commands as a dialog between you and the interpreter. To start, the interpreter prints a prompt, after which you type your command: +}; \\node (promptlabel) at (-3,0) {\\small\\sc Prompt}; \\draw [->] (promptlabel.east) -- (prompt.west); \\node (statement) at (1.5,0) [rectangle,fill=black!20] {\\verb+sqrt(25)+}; \\node (commandlabel) at (4,0) {\\small\\sc Command}; \\draw [->] (commandlabel.west) -- (statement.east); \\end{tikzpicture} ``` --> When you press “Enter,” the interpreter reads your command and performs the computation. For commands such as this one, the interpreter will print the output value from the computation: +}; \\node (statement) at (1.5,0) [rectangle,fill=black!20] {\\verb+sqrt(25)+}; \\node (output) at (1,-.75) {\\verb+[1] 5+}; \\node (outputlabel) at (4,-.75) {\\small\\sc Output Value}; \\node (outputmarker) at (-2.25, -.75) {\\small \\sc Output Marker}; \\draw [->] (outputmarker.east) -- (output.west); \\draw [->] (outputlabel.west) -- (output.east); \\node (prompt2) at (0, -1.5) [ellipse,fill=red!20] {\\verb+>+}; \\node (nextlabel) at (-2.5,-1.5) {\\small\\sc Next Prompt}; \\draw [->] (nextlabel.east) -- (prompt2.west); \\end{tikzpicture} ``` --> The dialog continues as the interpreter prints another prompt and waits for your further command. To save space, I’ll usually show just the give-and-take from one round of the dialog: sqrt(25) ## [1] 5 (Go ahead! Type sqrt(25) after the prompt in the R interpreter, press ``enter,’’ and see what happens.) Often, operations involve more than one argument. The various arguments are separated by commas. For example, here is an operation named seq that produces a sequence of numbers: seq(3,10) ## [1] 3 4 5 6 7 8 9 10 The first argument tells where to start the sequence, the second tells where to end it. The order of the arguments is important. Here is the sequence produced when 10 is the first argument and 3 the second: seq(10,3) ## [1] 10 9 8 7 6 5 4 3 For some operators, particularly those that have many input arguments, some of the arguments can be referred to by name rather than position. This is particularly useful when the named argument has a sensible default value. For example, the seq operator can be instructed how big a jump to take between successive items in the sequence. This is accomplished using an argument named by: seq(3,10,by=2) ## [1] 3 5 7 9 Depending on the circumstances, all four parts of a operation need not be present. For example, the date operation returns the current time and date; no input arguments are needed. date() ## [1] &quot;Sat Sep 3 12:12:16 2016&quot; Note that even though there are no arguments, the parentheses are still used. Think of the pair of parentheses as meaning, “Do this.” 1.3.1 Naming and Storing Values Often the value returned by an operation will be used later on. Values can be stored for later use with the assignment operator . This has a different syntax that reminds the user that a value is being stored. Here’s an example of a simple assignment: x &lt;- 16 This command has stored the value 16 under the name x. The syntax is always the same: an equal sign (=) with a name on the left and a value on the right. Such stored values are called objects . Making an assignment to an object defines the object. Once an object has been defined, it can be referred to and used in later computations. Notice that an assignment operation does not return a value or display a value. Its sole purpose is to have the side effects of defining the object and thereby storing a value under the object’s name. To refer to the value stored in the object, just use the object’s name itself. For instance: x ## [1] 16 Doing a computation on the value stored in an object is much the same: &lt;&lt;&gt;&gt;= sqrt(x) @ You can create as many objects as you like and give them names that remind you of their purpose. Some examples: wilma, ages, temp, dog.houses, foo3. There are some rules for object names: Use only letters and numbers and the two punctuation marks “dot” (.) and “underscore” (_). Do NOT use spaces anywhere in the name. A number or underscore cannot be the first character in the name. Capital letters are treated as distinct from lower-case letters. The objects named wilma and Wilma are different. For the sake of readability, keep object names short. But if you really must have an object named something like agesOfChildrenFromTheClinicalTrial, feel free. Objects can store all sorts of things, for example a sequence of numbers: x &lt;- seq(1,7) When you assign a new value to an existing object, as just done to x, the former value of that object is erased from the computer memory. The former value of x was 16, but after the above assignment command it is x ## [1] 1 2 3 4 5 6 7 The value of an object is changed only via the assignment operator. Using an object in a computation does not change the value. For example, suppose you invoke the square-root operator on x: sqrt(x) ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 The square roots have been returned as a value, but this doesn’t change the value of x: x ## [1] 1 2 3 4 5 6 7 If you want to change the value of x, you need to use the assignment operator: x &lt;- sqrt(x) x ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 1.3.2 Assignment vs Algebra An assignment command like x = sqrt(x) can be confusing to people who are used to algebraic notation. In algebra, the equal sign describes a relationship between the left and right sides. So, \\(x = \\sqrt{x}\\) tells us about how the quantity \\(x\\) and the quantity \\(\\sqrt{x}\\) are related. Students are usually trained to “solve” such relationships, going through a series of algebraic steps to find values for \\(x\\) that are consistent with the mathematical statement. (For \\(x = \\sqrt{x}\\), the solutions are \\(x=0\\) and \\(x=1\\).) In contrast, the assignment command x = sqrt(x) is a way of replacing the previous values stored in x with new values that are the square root of the old ones. 1.3.3 Connecting Computations The brilliant thing about organizing operators in terms of input arguments and output values is that the output of one operator can be used as an input to another. This lets complicated computations be built out of simpler ones. For example, suppose you have a list of 10000 voters in a precinct and you want to select a random sample of 20 of them for a survey. The seq operator can be used to generate a set of 10000 choices. The sample operator can be used to select some of these choices at random. One way to connect the computations is by using objects to store the intermediate outputs. choices = seq(1,10000) sample( choices, 20 ) ## [1] 9831 9797 9845 4901 4270 7064 2983 9339 5801 9553 111 7027 3502 3423 ## [15] 6580 9858 9232 938 204 6090 You can also pass the output of an operator directly as an argument to another operator. Here’s another way to accomplish exactly the same thing as the above. sample( seq(1,10000), 20 ) ## [1] 6228 4446 7432 4398 2591 7417 3058 7853 872 4528 2149 5138 9200 5532 ## [15] 7369 1582 4311 4098 1675 9577 1.3.4 Numbers and Arithmetic The language has a concise notation for arithmetic that looks very much like the traditional one: 7+2 ## [1] 9 3*4 ## [1] 12 5/2 ## [1] 2.5 3-8 ## [1] -5 -3 ## [1] -3 5^2 ## [1] 25 Arithmetic operators, like any other operators, can be connected to form more complicated computations. For instance, 8+4/2 ## [1] 10 To a human reader, the command 8+4/2 might seem ambiguous. Is it intended to be (8+4)/2 or 8+(4/2)? The computer uses unambiguous rules to interpret the expression, but it’s a good idea for you to use parethesis so that you can make sure that what you intend is what the computer carries out: (8+4)/2 ## [1] 6 Traditional mathematical notation uses superscripts and radicals to indicate exponentials and roots, e.g., \\(3^2\\) or \\(\\sqrt{3}\\) or \\(\\sqrt[3]{8}\\). This special typography doesn’t work well with an ordinary keyboard, so R and most other computer languages uses a different notation: 3^2 ## [1] 9 sqrt(3) ## [1] 1.732051 8^(1/3) ## [1] 2 There is a large set of mathematical functions: exponentials, logs, trigonometric and inverse trigonometric functions, etc. Some examples: Traditional Computer \\(e^2\\) exp(2) \\(\\log_e(100)\\) log(100) \\(\\log_{10}(100)\\) log10(100) \\(\\log_{2}(100)\\) log2(100) \\(\\cos( \\frac{\\pi}{2})\\) cos(pi/2) \\(\\sin( \\frac{\\pi}{2})\\) sin(pi/2) \\(\\tan( \\frac{\\pi}{2})\\) tan(pi/2) \\(\\cos^{-1}(-1)\\) acos(-1) Numbers can be written in scientific notation. For example, the “universal gravitational constant” that describes the gravitational attraction between masses is \\(6.67428 \\times 10^{-11}\\) (with units meters-cubed per kilogram per second squared). In the computer notation, this would be written G=6.67428e-11. The Avogradro constant, which gives the number of atoms in a mole, is \\(6.02214179 \\times 10^{23}\\) per mole, or 6.02214178e23. The computer language does not directly support the recording of units. This is unfortunate, since in the real world numbers often have units and the units matter. For example, in 1999 the Mars Climate Orbiter crashed into Mars because the design engineers specified the engine’s thrust in units of pounds, while the guidance engineers thought the units were newtons. Computer arithmetic is accurate and reliable, but it often involves very slight rounding of numbers. Ordinarily, this is not noticeable. However, it can become apparent in some calculations that produce results that are zero. For example, mathematically \\(\\sin(\\pi) = 0\\), however the computer does not duplicate this mathematical relationship exactly: sin(pi) ## [1] 1.224647e-16 Whether a number like this is properly interpreted as “close to zero,” depends on the context and, for quantities that have units, on the units themselves. For instance, the unit “parsec” is used in astronomy in reporting distances between stars. The closest star to the sun is Proxima, at a distance of 1.3 parsecs.2 A distance of \\(1.22 \\times 10^{-16}\\) parsecs is tiny in astronomy but translates to about 2.5 meters — not so small on the human scale. In statistics, many calculations relate to probabilities which are always in the range 0 to 1. On this scale, 1.22e-16 is very close to zero. There are two “special” numbers. Inf stands for \\(\\infty\\), as in 1/0 ## [1] Inf NaN stands for “not a number,” and is the result when a numerical operation isn’t defined, for instance 0/0 ## [1] NaN 1.3.5 Aside: Complex numbers Mathematically oriented readers will wonder why R should have any trouble with a computation like \\(\\sqrt{-9}\\); the result is the imaginary number \\(3i\\). R works with complex numbers, but you have to tell the system that this is what you want to do. To calculate \\(\\sqrt{-9}\\), use . 1.3.6 Types of Objects Most of the examples used so far have dealt with numbers. But computers work with other kinds of information as well: text, photographs, sounds, sets of data, and so on. The word type is used to refer to the kind of information. Modern computer languages support a great variety of types. It’s important to know about the types of data because operators expect their input arguments to be of specific types. When you use the wrong type of input, the computer might not be able process your command. For the purpose of starting with R, it’s important to distinguish among three basic types: numeric The numbers of the sort already encountered. data frames Collections of data more or less in the form of a spreadsheet table. The Computation Technique section in Chapter @ref(“chap:data-cases-variables”) introduces the operators for working with data frames. character Text data. You indicate character data to the computer by enclosing the text in double quotation marks. For example: filename = &quot;swimmers.csv&quot; There is something a bit subtle going on in the above command, so look at it carefully. The purpose of the command is to create an object, named filename, that stores a little bit of text data. Notice that the name of the object is not put in quotes, but the text characters are. Whenever you refer to an object name, make sure that you don’t use quotes, for example: filename ## [1] &quot;swimmers.csv&quot; If you make a command with the object name in quotes, it won’t be treated as referring to an object. Instead, it will merely mean the text itself: &quot;filename&quot; ## [1] &quot;filename&quot; Similarly, if you omit the quotation marks from around text, the computer will treat it as if it were an object name and will look for the object of that name. For instance, the following command directs the computer to look up the value contained in an object named swimmers.csv and insert that value into the object filename. filename &lt;- swimmers.csv ## Error in eval(expr, envir, enclos): object &#39;swimmers.csv&#39; not found As it happens, there was no object named swimmers.csv because it had not been defined by any previous assignment command. So, the computer generated an error. For the most part, you will not need to use very many operators on text data; you just need to remember to include text, such as file names, in quotation marks, &quot;like this&quot;. DRAFT: Look how out of date this is! No notion of RStudio. Do note that in the knitr chunk, the chunk label (“console-picture”) becomes part of the label for the picture, which can be put in with \\@ref(console-picture) as in Figure ??.↩ News from August 2016: Astronomers have announced that Proxima has a planet orbiting it in the “habitable zone.”↩ "],
["data-in-r.html", "Chapter 2 Data in R 2.1 Preliminaries 2.2 Reading Tabular Data into R 2.3 Data Frames 2.4 Variables in Data Frames 2.5 Adding a New Variable 2.6 Sampling from a Sample Frame", " Chapter 2 Data in R 2.1 Preliminaries Many of the R functions used in this text are written especially for the text to enhance convienence and clarity of purpose. To access these functions, you will need to load the mosaic package at the beginning of your session. Loading a package is simple: require(mosaic) You need do this only once in each session of R, and on systems such as Rstudio the package will generally be reloaded automatically. (If you get an error message, it’s likely that the mosaic package has not been installed on your system. Use the package installation menu in R to install mosaic, after which the require() function will load the package.) mosaic itself loads other packages it in turn depends on. If a command you see in this text does not work for you, be sure that mosaic is loaded. Data used in statistical modeling are usually organized into tables, often created using spreadsheet software. Most people presume that the same software used to create a table of data should be used to display and analyze it. This is part of the reason for the popularity of spreadsheet programs such as Excel and Google Spreadsheets. For serious statistical work, it’s helpful to take another approach that strictly separates the processes of data collection and of data analysis: use one program to create data files and another program to analyze the data stored in those files. By doing this, one guarantees that the original data are not modified accidentally in the process of analyzing them. This also makes it possible to perform many different analyses of the data; modelers often create and compare many different models of the same data. 2.2 Reading Tabular Data into R Data is central to statistics, and the tabular arrangement of data is very common. Accordingly, R provides a large number of ways to read in tabular data. These vary depending on how the data are stored, where they are located, etc, but they generally take on similar forms which will become familiar to you with use. This text makes use of several datasets and most of these are available to you in the package mosaicData. You can load this into your workspace with the now familiar command require(mosaicData) or by checking the box next to mosaicData in the packages tab in Rstudio. Once this is done, you can refer to a dataset by its name in mosaicData (clicking on mosaicData in the packages tab in Rstudio will bring up an index of names and associated codebooks). An often used classic dataset residing in mosaicData is the height data collected by Sir Francis Galton. In the following commands look at the first few records: head(Galton) ## family father mother sex height nkids ## 1 1 78.5 67.0 M 73.2 4 ## 2 1 78.5 67.0 F 69.2 4 ## 3 1 78.5 67.0 F 69.0 4 ## 4 1 78.5 67.0 F 69.0 4 ## 5 2 75.5 66.5 M 73.5 4 ## 6 2 75.5 66.5 M 72.5 4 Though mosaicData contains many of our text’s datasets, it does not contain all of them, and you’ll be wanting to analyse your own data, generated and then stored in tabular form. The most common method of reading tabular data, for the purposes of this book, is the R operator read.csv() which, not surprisingly, reads in .csv or comma separated variable files. These are text files that can be generated by a spreadsheet. read.csv() imports tabular data (in .csv format) into R from anywhere on your computer or on the web. Reading in a data table that’s been connected with read.csv() is simply a matter of knowing the name of the data set. For instance, one data table used in examples in this book is swim100m.csv. All of the .csv files of data mentioned in the text are available on the web at http://tinyhttp://tiny.cc/mosaic/ so to read in this data table and create an object in R that contains the data, use a command like this: Swim &lt;- read.csv(&quot;http://tiny.cc/mosaic/swim100m.csv&quot;) The part of this command that requires creativity is choosing a name for the R object that will hold the data. In the above command it is called Swim, but you might prefer another name, e.g., S or Sdata or even Ralph. Beginning with a capital letter is standard practice, but not required. Remember, R is case sensitive. Of course, it’s sensible to choose names that are short, easy to type and remember, and remind you what the contents of the object are about. To help you identify data tables that can be accessed through read.csv(), examples in this book will be marked with a flag containing the name of the file. 2.3 Data Frames The type of R object created by read.csv() is called a data frame and is essentially a tabular layout. To illustrate , here are the first several cases of the Swim data frame created by the previous use of read.csv(): head(Swim) ## year time sex ## 1 1905 65.8 M ## 2 1908 65.6 M ## 3 1910 62.8 M ## 4 1912 61.6 M ## 5 1918 61.4 M ## 6 1920 60.4 M What do you think a function might be called that prints out the last several cases? Try it. Note that the head() function, one of several functions that operate on data frames, takes the R object that you created, not the quoted name of the data file. Data frames, like tabular data generally, involve variables and cases. In R, each of the variables is given a name. You can refer to the variable by name in a couple of different ways. To see the variable names in a data frame, something you might want to do to remind yourself of how names are spelled and capitalized, use the names() function: names(Swim) ## [1] &quot;year&quot; &quot;time&quot; &quot;sex&quot; Another way to get quick information about the variables in a data frame is with summary(): summary(Swim) ## year time sex ## Min. :1905 Min. :47.84 F:31 ## 1st Qu.:1924 1st Qu.:53.64 M:31 ## Median :1956 Median :56.88 ## Mean :1952 Mean :59.92 ## 3rd Qu.:1976 3rd Qu.:65.20 ## Max. :2004 Max. :95.00 To see how many cases there are in a data frame, use nrow(): nrow(Swim) ## [1] 62 2.4 Variables in Data Frames Perhaps the most common operation on a data frame is to refer to the values in a single variable. The two ways you will most commonly use involve functions with a data = argument and the direct use of the $ notation. The $ notation is the most basic, if not the most intuitive, way of referring to a variable in a dataframe. Here we find the mean record time (time) in the dataset we’ve named Swim: mean(Swim$time) ## [1] 59.92419 Think of this as referring to the variable by both its family name (the data frame’s name,Swim) and its given name (time), something like Clinton$Hillary. Most of the statistical modeling functions you will encounter in this book are designed to work with data frames and allow you to refer directly to variables within a data frame. For instance: mean( ~ time, data = Swim) ## [1] 59.92419 min( ~ time, data = Swim) ## [1] 47.84 The data = argument tells the function which data frame to pull the variable from. The use of the tilde (~) identifies the first argument as a model formula, which is necessary if the data = argument is to be used. Leaving off that argument or the tilde leads to an error. The advantage of the data = approach becomes evident when you construct statements that involve more than one variable within a data frame. For instance, here’s a calculation of the mean time separately for the different sexes: mean( time ~ sex, data = Swim ) ## F M ## 65.19226 54.65613 Alternatively, mean( Swim$time ~ Swim$sex ) ## F M ## 65.19226 54.65613 You will see much more of the tilde starting in Chapter @ref(“chap:simple-models”). It’s the R notation for “broken down by” or “versus.” The ability of mean(), median(), and similar functions to handle the data = format is provided by the mosaic package. When you encounter a function that can’t handle the data = format, use the $ notation. 2.5 Adding a New Variable Sometimes you will compute a new quantity from the existing variables and want to treat this as a new variable. Adding a new variable to a data frame can be done with the $ notation. For instance, here is how to create a new variable in Swim that holds the time converted from minutes to units of seconds: Swim$minutes = Swim$time/60 The new variable appears just like the old ones: head(Swim, n = 3L) ## year time sex minutes ## 1 1905 65.8 M 1.096667 ## 2 1908 65.6 M 1.093333 ## 3 1910 62.8 M 1.046667 You could also, if you want, redefine an existing variable, for instance: Swim$time = Swim$time/60 head(Swim, n = 3L) ## year time sex minutes ## 1 1905 1.096667 M 1.096667 ## 2 1908 1.093333 M 1.093333 ## 3 1910 1.046667 M 1.046667 Such assignment operations do not change the original file (e.g. the swim100m.csv file) from which the data were read, only the data frame in the current session of R. This is an advantage, since it means that your data in the data file stay in their original state and therefore won’t be corrupted by operations made during analysis. 2.6 Sampling from a Sample Frame Much of statistical analysis is concerned with the consequences of drawing a sample from the population. Ideally, you will have a sampling frame that lists every member of the population from which the sample is to be drawn. With this in hand, you could treat the individual cases in the sampling frame as if they were cards in a deck of hands. To pick your random sample, shuffle the deck and deal out the desired number of cards. When doing real work in the field, you would use the randomly dealt cards to locate the real-world cases they correspond to. Sometimes in this book, however, in order to let you explore the consequences of sampling, you will select a sample from an existing data set. The deal() function performs this, taking as an argument the data frame to be used in the selection and the number of cases to sample. For example, the kidsfeet.csv data set has \\(n=39\\) cases. Kids &lt;- read.csv(&quot;http://tiny.cc/mosaic/kidsfeet.csv&quot;) nrow(Kids) ## [1] 39 Here’s how to take a random sample of five of the cases: deal(Kids, 5) ## name birthmonth birthyear length width sex biggerfoot domhand orig.id ## 23 Laura 9 88 24.0 8.3 G R L 23 ## 19 Lee 6 88 26.7 9.0 G L L 19 ## 29 Mike 11 88 24.2 8.9 B L R 29 ## 20 Heather 3 88 25.5 9.5 G R R 20 ## 4 Josh 1 88 25.2 9.8 B L R 4 The results returned by deal() will never contain the same case more than once, just as if you were dealing cards from a shuffled deck. In contrast, resample replaces each case after it is dealt so that it can appear more than once in the result. You wouldn’t want to do this to select from a sampling frame, but it turns out that there are valuable statistical uses for this sort of sampling with replacement. . You’ll make use of re-sampling in Chapter ??. resample(Kids, 5) ## name birthmonth birthyear length width sex biggerfoot domhand orig.id ## 17 Caroline 12 87 24.0 8.7 G R L 17 ## 39 Alisha 9 88 24.6 8.8 G L R 39 ## 13 Cal 8 87 26.1 9.1 B L R 13 ## 35 Peter 4 88 24.7 8.6 B R L 35 ## 17.1 Caroline 12 87 24.0 8.7 G R L 17 Notice that Caroline was sampled twice. "],
["describing-variation.html", "Chapter 3 Describing Variation 3.1 Simple Statistical Calculations 3.2 Simple Statistical Graphics 3.3 Displays of Categorical Variables", " Chapter 3 Describing Variation As a setting to illustrate computer techniques for describing variability, take the data that Galton collected on the heights of adult children and their parents. These data, in a modern case/variable format, are made available as Galton when the mosaic package is used. require(mosaic) Galton = read.csv(&quot;http://tiny.cc/mosaic/galton.csv&quot;) # As it happens, Galton is already loaded by mosaic from the mosaicData package. # So you don&#39;t need the read.csv(); we use it here for the sake of generality. 3.1 Simple Statistical Calculations Simple numerical descriptions are easy to compute. Here are the mean, median, standard deviation and variance of the children’s heights (in inches). # [Always a space on both sides of an operator] mean( ~ height, data = Galton) ## [1] 66.76069 median( ~ height, data = Galton) ## [1] 66.5 sd( ~ height, data = Galton) ## [1] 3.582918 var( ~ height, data = Galton) ## [1] 12.8373 Notice that the variance function var() returns the square of the standard deviation produced by sd(). Having both is merely a convenience. Why the tilde? In all these commands the first argument (~ height) is recognized by the interpreter as a model formula. Model language involves the tilde, and the tilde must be followed by a variable. 3.1.1 Aside: The “base” versions The built-in, base R version of commands such as mean(), median(), sd(), and so on (that is, in their default non-mosaic form) take numeric vectors (columns of numbers) as arguments, but then the convenience of the data = designation is not available. For instance: mean(Galton$height) ## [1] 66.76069 median(Galton$height) ## [1] 66.5 As we have seen before, R’s model language also allows more sophisticated model arguments, as in median(height ~ sex, data = Galton) ## F M ## 64.0 69.2 A percentile tells where a given value falls in a distribution. For example, a height of 63 inches is on the short side in Galton’s data: pdata( ~ height, 63, data = Galton) ## [1] 0.1915367 Only about 19 % of the cases have a height less than or equal to 63 inches. The pdata operator takes one or more values as a second argument (here, 63) and finds where they fall in the distribution of values in the first argument (height). A quantile refers to the same sort of calculation, but inverted. Instead of giving a value in the same units as the distribution, you give a probability: a number between 0 and 1. The qdata operator then calculates the value whose percentile would be that value. What’s the 20th percentile of Galton’s heights? qdata( ~ height, 0.2, data = Galton) ## p quantile ## 0.2 63.5 Remember that the probability is given as a number between 0 and 1, so use 0.50 to indicate that you want the value which falls at the 50th percentile. qdata( ~ height, 0.5, data = Galton) ## p quantile ## 0.5 66.5 median( ~ height, data = Galton) ## [1] 66.5 The 25th and 75th percentile in a single command — in other words, the 50 percent coverage interval: # [Note: formula should be first argument.] qdata( ~ height, c(0.25, 0.75), data = Galton) ## quantile p ## 25% 64.0 0.25 ## 75% 69.7 0.75 The 2.5th and 97.5th percentile — in other words, the 95 percent coverage interval: qdata( ~ height, c(0.025, 0.975), data = Galton) ## quantile p ## 2.5% 60 0.025 ## 97.5% 73 0.975 The interquartile range is the width of the 50 percent coverage interval: IQR( ~ height, data = Galton) ## [1] 5.7 Some other useful operators are min(), max(), and range(). 3.2 Simple Statistical Graphics There are several basic types of statistical graphics to display the distribution of a variable: histograms, density plots, and boxplots. These are easily mastered by example. 3.2.1 Histograms and Distributions Constructing a histogram involves dividing the range of a variable up into bins and counting how many cases fall into each bin. This is done in an almost entirely automatic way: histogram( ~ height, data = Galton) When constructing a histogram, R makes an automatic but sensible choice of the number of bins. If you like, you can control this yourself. For instance: histogram( ~ height, data = Galton, breaks = 25 ) The horizontal axis of the histogram is always in the units of the variable. For the histograms above, the horizontal axis is in “inches” because that is the unit of the height variable. The vertical axis is conventionally drawn in one of three ways: controlled by an optional argument named type. Absolute Frequency or Counts A simple count of the number of cases that falls into each bin. This mode is set with type=&quot;count&quot; as in histogram( ~ height, data = Galton, type = &quot;count&quot;) Figures/variation-var3-hist Relative Frequency The vertical axis is scaled so that the height of the bar give the proportion of cases that fall into the bin. This is the default, that is, this is the type if you don’t specify type in the command. Density The vertical axis area of the bar gives the relative proportion of cases that fall into the bin. Set type = &quot;density&quot;. In a density plot, areas can be interpreted as probabilities and the area under the entire histogram is equal to 1. Other useful optional arguments set the labels for the axes and the graph as a whole and color the bars. For example, histogram( ~ height, data = Galton, type = &quot;density&quot;, xlab=&quot;Height (inches)&quot;, main=&quot;Distribution of Heights&quot;, col=&quot;gray&quot;) The above command is so long that it has been broken into several lines for display purposes. R ignores the line breaks, holding off on executing the command until it sees the final closing parentheses. Notice the use of quotation marks to delimit the labels and names like &quot;blue&quot;. Also note the + signs that appear in place of the prompt (&gt;) on new lines when the command is still incomplete. If a plus sign appears when you are expecting a prompt, R is telling you that the previous command is incomplete. You need to complete it before you get back to the prompt. 3.2.2 Density Plots A density plot avoids the need to create bins and plots out the distribution as a continuous curve. Making a density plot involves two operators. The density operator performs the basic computation which is then displayed using either the plot or the lines operator. For example: densityplot( ~ height, data = Galton) If you want to suppress the rug-like plotting of points at the bottom of the graph, use densityplot( ~ height, data = Galton, plot.points = FALSE). 3.2.3 Box-and-Whisker Plots Box-and-whisker plots are made with the bwplot command: bwplot( ~ height, data = Galton) The median is represented by the heavy dot in the middle. Outliers, if any, are marked by dots outside the whiskers. The real power of the box-and-whisker plot is for comparing distributions. This will be raised again more systematically in later chapters, but just to illustrate, here is how to compare the heights of males and females: bwplot(height ~ sex, data = Galton) 3.3 Displays of Categorical Variables For categorical variables, it makes no sense to compute descriptive statistics such as the mean, standard deviation, or variance. Instead, look at the number of cases at each level of the variable. tally( ~ sex, data = Galton) ## ## F M ## 433 465 Proportions can be found by dividing the tallies by the total: tally( ~ sex, data = Galton)/nrow(Galton) ## ## F M ## 0.4821826 0.5178174 "],
["groupwise-models.html", "Chapter 4 Groupwise models 4.1 Model Values and Residuals", " Chapter 4 Groupwise models Calculating means and other simple statistics is a matter of using the right function in R. The mosaic package — which you should load in to R as shown in Section @ref(“sec:loading-mosaic”) — makes it straightforward to calculate either a “grand” statistic or a “group-wise” statistic. To illustrate: Load the mosaic package, needed only once per session. Read in data you are interested in analyzing, for instance the Cherry-Blossom 2008 data described earlier:3 Runners = read.csv(&quot;http://tiny.cc/mosaic/Cherry-Blossom-2008.csv&quot;) names( Runners ) ## [1] &quot;position&quot; &quot;division&quot; &quot;total&quot; &quot;name&quot; &quot;age&quot; &quot;place&quot; &quot;net&quot; &quot;gun&quot; ## [9] &quot;sex&quot; Calculate a grand mean on the “gun” time — the time between the start of the race, signalled by a gun, and when each runner crossed the finish line. Here, to emphasize that this is a group–wise model, we use the function gwm() from the mosaic package. It works the same as mean(), but its output is more formally the output of a model. gwm(gun ~ 1, data = Runners) ## ## Groupwise Model Call: ## gwm(formula = gun ~ 1, data = Runners) ## ## model_value ## 1 93.73 The model formula gun ~ 1 is an “all cases the same” model. You will refer to the 1 as the “Intercept term” later in the text, but for now, think of it as “one value.” The one value here is the grand mean. Grand mean and other “grand” statistics. mean( gun ~ 1, data = Runners) ## 1 ## 93.72504 median( gun ~ 1, data = Runners ) ## 1 ## 93.68333 sd( gun ~ 1, data = Runners ) ## 1 ## 14.96899 To tell R that you want to break the statistics down by groups, replace the 1 with the variable which defines the groups. You will be using this notation frequently in building models. Here, as before, the ~ means, “model by” or “broken down by” or “versus.” To find the means for the gun time broken down by sex, enter gwm( gun ~ sex, data = Runners ) ## ## Groupwise Model Call: ## gwm(formula = gun ~ sex, data = Runners) ## ## sex model_value ## 1 F 98.77 ## 2 M 88.26 Other statistics work the same way, for instance, sd( gun ~ sex, data = Runners ) ## F M ## 13.33713 14.71851 Another example … wage broken down by sector of the economy, using data cps.csv:4 CPS = read.csv(&quot;http://tiny.cc/mosaic/cps.csv&quot;) gwm( wage ~ sector, data = CPS ) ## ## Groupwise Model Call: ## gwm(formula = wage ~ sector, data = CPS) ## ## sector model_value ## 1 clerical 7.423 ## 2 const 9.502 ## 3 manag 12.704 ## 4 manuf 8.036 ## 5 other 8.501 ## 6 prof 11.947 ## 7 sales 7.593 ## 8 service 6.537 In the Whickham smoking data example5, the outcome for each person was not a number but a category: Alive or Dead at the time of the follow-up. W &lt;- read.csv(&quot;http://tiny.cc/mosaic/whickham.csv&quot;) names(W) ## [1] &quot;outcome&quot; &quot;smoker&quot; &quot;age&quot; levels(W$outcome) ## [1] &quot;Alive&quot; &quot;Dead&quot; The gwm() function recognizes that outcome is a categorical variable and outputs proportions. In the case of “all cases the same,” this is porportions of the data in the outcome levels: gwm( outcome ~ 1, data = W ) ## ## Groupwise Model Call: ## gwm(formula = outcome ~ 1, data = W) ## ## outcome model_value ## 1 Alive 0.7192 ## 2 Dead 0.2808 Here’s the breakdown of the levels Alive and Dead according to smoking status: gwm( outcome ~ smoker, data = W ) ## ## Groupwise Model Call: ## gwm(formula = outcome ~ smoker, data = W) ## ## outcome smoker model_value ## 1 Alive No 0.3820 ## 2 Dead No 0.1750 ## 3 Alive Yes 0.3371 ## 4 Dead Yes 0.1058 You may be surbrised by this result. But don’t be misled. A more meaningful question is whether smokers are different from non-smokers when holding other variables constant, such as age. To address this question, you need to add age into the model. It might be natural to consider each age — 35, 36, 37, and so on — as a separate group, but you won’t get very many members of each group. And, likely, the data for 35 year-olds has quite a lot to say about 36 year-olds, so it doesn’t make sense to treat them as completely separate groups. You can use the cut() function to divide up a quantitative variable into groups. You get to specify the breaks between groups. W$ageGroups &lt;- cut(W$age, breaks=c(0,30,40,53,64,75,100) ) gwm( outcome ~ ageGroups, data = W ) ## ## Groupwise Model Call: ## gwm(formula = outcome ~ ageGroups, data = W) ## ## outcome ageGroups model_value ## 1 Alive (0,30] 0.214612 ## 2 Dead (0,30] 0.004566 ## 3 Alive (30,40] 0.181887 ## 4 Dead (30,40] 0.009893 ## 5 Alive (40,53] 0.177321 ## 6 Dead (40,53] 0.035769 ## 7 Alive (53,64] 0.119482 ## 8 Dead (53,64] 0.071537 ## 9 Alive (64,75] 0.025875 ## 10 Dead (64,75] 0.102740 ## 11 Alive (75,100] 0.000000 ## 12 Dead (75,100] 0.056317 gwm( outcome ~ smoker + ageGroups, data = W ) ## ## Groupwise Model Call: ## gwm(formula = outcome ~ smoker + ageGroups, data = W) ## ## outcome smoker ageGroups model_value ## 1 Alive No (0,30] 0.123288 ## 2 Dead No (0,30] 0.002283 ## 3 Alive Yes (0,30] 0.091324 ## 4 Dead Yes (0,30] 0.002283 ## 5 Alive No (30,40] 0.097412 ## 6 Dead No (30,40] 0.004566 ## 7 Alive Yes (30,40] 0.084475 ## 8 Dead Yes (30,40] 0.005327 ## 9 Alive No (40,53] 0.075342 ## 10 Dead No (40,53] 0.010654 ## 11 Alive Yes (40,53] 0.101979 ## 12 Dead Yes (40,53] 0.025114 ## 13 Alive No (53,64] 0.064688 ## 14 Dead No (53,64] 0.031963 ## 15 Alive Yes (53,64] 0.054795 ## 16 Dead Yes (53,64] 0.039574 ## 17 Alive No (64,75] 0.021309 ## 18 Dead No (64,75] 0.078387 ## 19 Alive Yes (64,75] 0.004566 ## 20 Dead Yes (64,75] 0.024353 ## 21 Alive No (75,100] 0.000000 ## 22 Dead No (75,100] 0.047184 ## 23 Alive Yes (75,100] 0.000000 ## 24 Dead Yes (75,100] 0.009132 With modeling techniques, to be introduced in later chapters, you can use quantitative variables without the need to divide them into groups. 4.1 Model Values and Residuals A group-wise model tells you a model value for each group, but often you will need these in a case-by-case format: the model value for each case in a data set. The fitted() function carries out this simple calculation, taking each case in turn, figuring out which group it belongs to, and then returning the set of model values for all the cases. It requires two arguments: the model (here provided by gwm()) the data on which the model was based. For example: Kids &lt;- KidsFeet # just get this from mosaicData instead of going out on the web with read.csv mod &lt;- gwm( width ~ sex, data = Kids ) fitted( mod, Kids ) ## [1] 9.190000 9.190000 9.190000 9.190000 9.190000 9.190000 9.190000 8.784211 8.784211 9.190000 ## [11] 9.190000 9.190000 9.190000 9.190000 8.784211 8.784211 8.784211 8.784211 8.784211 8.784211 ## [21] 9.190000 9.190000 8.784211 8.784211 8.784211 9.190000 8.784211 9.190000 9.190000 9.190000 ## [31] 8.784211 8.784211 8.784211 9.190000 9.190000 8.784211 8.784211 8.784211 8.784211 The residuals are found by subtracting the case-by-case model value from the actual values for each case. res &lt;- Kids$width - fitted(mod, Kids) head(res) # display the first few residuals ## [1] -0.79 -0.39 0.51 0.61 -0.29 0.51 res &lt;- resid(mod, Kids) # function resid does the calculation automatically head(res) ## [1] -0.79 -0.39 0.51 0.61 -0.29 0.51 Take care to use the same quantitative variable (width in this case) from the data as was used in constructing the model. The var() function will calculate the variance: var( Kids$width ) # overall variation ## [1] 0.2596761 var( fitted(mod, Kids) ) # variation in model values ## [1] 0.04222182 var( resid(mod, Kids) ) # residual variation ## [1] 0.2174543 Notice the “model triangle” relationship between these three numbers. These are also made available in the mosaic package as TenMileRace.↩ This data set is made available by mosaic as CPS85.↩ This data set is made available by mosaic as Whickham.↩ "],
["confidence-intervals.html", "Chapter 5 Confidence intervals 5.1 Finding a Sampling Distribution through Bootstrapping 5.2 Computing Grade-Point Averages", " Chapter 5 Confidence intervals Again, you will need the mosaic package in R, which provides some of the basic operators for constructing sampling and resampling distributions. require(mosaic) The examples will be based on the Cherry-Blossom 2008 data described earlier: Runners = read.csv(&quot;http://tiny.cc/mosaic/Cherry-Blossom-2008.csv&quot;) names( Runners ) ## [1] &quot;position&quot; &quot;division&quot; &quot;total&quot; &quot;name&quot; &quot;age&quot; &quot;place&quot; &quot;net&quot; &quot;gun&quot; ## [9] &quot;sex&quot; 5.1 Finding a Sampling Distribution through Bootstrapping Your data are typically a sample from a population. Collecting the sample is usually hard work. Just to illustrate the context of a sampling distribution, here is a simulation of selecting a sample of size \\(n=100\\) from the population of runners. It’s essential to keep in mind that you do not usually pull out your sample using the computer in this way. Instead, you go into the field or laboratory and collect your data. Mysamp = deal( Runners, 100 ) # A Simulation of sampling Now that you have a sample, you can calculate the sample statistic that’s of interest to you. For instance: mean( gun ~ sex, data = Mysamp ) ## F M ## 100.20606 90.88333 Note that the results are slightly different from those found above using the whole population. That’s to be expected, since the sample is just a random part of the population. But ordinarily, you will not know what the population values are; all you have to work with is your sample. Theoretically, the sampling distribution reflects the variation from one randomly dealt sample to another, where each sample is taken from the population. In practice, your only ready access to the population is through your sample. So, to simulate the process of random sampling, re-sampling is used and the re-sampling distribution is used as a convenient approximation to the sampling distribution. Re-sampling involves drawing from the set of cases in your sample with replacement. To illustrate, consider this example of a very small, simple set: the numbers 1 to 5: nums = c(1,2,3,4,5) nums ## [1] 1 2 3 4 5 Each resample of size \\(n\\) consists of \\(n\\) members from the set, but in any one resample each member might appear more than once and some might not appear at all. Here are three different resamples from nums: resample(nums) ## [1] 3 1 3 1 3 resample(nums) ## [1] 4 3 5 4 3 resample(nums) ## [1] 3 2 1 2 3 To use resampling to estimate the sampling distribution of a statistic, you apply the calculation of the statistic to a resampled version of your sample. For instance, here are the group-wise means from one resample of the running sample: mean( gun ~ sex, data = resample(Mysamp) ) ## F M ## 99.45656 89.98291 And here is another: mean( gun ~ sex, data = resample(Mysamp) ) ## F M ## 101.67719 90.89845 The bootstrap procedure involves conducting many such trials and examining the variation from one trial to the next. The do() function lets you automate the collection of multiple trials. For instance, here are five trials carried out using do(): do(5) * mean( gun ~ sex, data = resample(Mysamp) ) ## F M ## 1 100.0438 89.75081 ## 2 100.8766 93.90732 ## 3 100.7636 93.49943 ## 4 101.2549 92.48796 ## 5 100.3470 94.47237 Typically, you will use several hundred trials for bootstrapping. The most common way to summarize the variation in bootstrap trials, you can calculate a coverage interval. (When applied to a sampling distribution, the coverage interval is called a confidence interval.) To do the computation, give a name to the results of the repeated bootstrap trials, here it’s called trials: trials = do(500) * mean( gun ~ sex, data = resample(Mysamp) ) head(trials) ## F M ## 1 100.8126 92.91923 ## 2 100.0116 89.11071 ## 3 100.9795 88.42286 ## 4 101.6498 95.82644 ## 5 103.3579 88.95704 ## 6 101.7951 88.94905 Computing the coverage intervals can be done using qdata() on the columns, or, for convenience, use the confint() function. confint(trials) ## name lower upper level method estimate ## 1 F 97.24245 103.13737 0.95 percentile 100.20606 ## 2 M 86.59421 96.22077 0.95 percentile 90.88333 To get a visual depiction of the sampling distribution, look at a histogram of Female mean gun time from the resamapled data. histogram(trials$F) Verify that the confidence interval for females calculated by confint() is the 95% coverage interval of trials$F as seen in the histogram. The idea of sampling distributions is based on drawing at random from the population, not resampling. Ordinarily, you can’t do this calculation since you don’t have the population at hand. But in this example, we happen to have the data for all runners. Here’s the population-based confidence interval for the mean running time, with sample size \\(n=100\\), broken down by sex: trials = do(500) * mean( gun ~ sex, data = deal(Runners, 100) ) confint(trials) ## name lower upper level method estimate ## 1 F 94.75541 102.35947 0.95 percentile 99.34655 ## 2 M 84.40595 92.47707 0.95 percentile 86.19722 The output of confint() shows the lower and upper limits of the confidence interval for each of the groups. The labels on the columns indicate the confidence level. By default, the interval is at the 95% level, and so the interval runs from the 2.5 percentile to the 97.5 percentile. Historically, statisticians have been concerned with very small samples: say \\(n=2\\) or \\(n=3\\). Even in this era of huge data sets, such small sample sizes often are encountered in laboratory experiments, etc. Bootstrapping cannot work well with such small samples, and other techniques are needed to simulate sampling variability. Many of these techniques are based in algebra and probability theory, and give somewhat complex formulas for calculating confidence intervals from data. The formulas are often found in textbooks, but for most of the modeling techniques you will use in later chapters, appropriate formulas for confidence intervals have been implemented in software. For other modeling techniques, bootstrapping is used to find the confidence intervals. But keep in mind that bootstrapping can only be effective when the sample size \\(n\\) is one or two dozen or larger. 5.2 Computing Grade-Point Averages The grade-point average is a kind of group-wise mean, where the group is an individual student. This is not the usual way of looking at things for a student, who sees only his or her own grades. But institutions have data on many students. The data files grades.csv and courses.csv are drawn from an institutional database at a college. They give the grades for more than 400 students who graduated in year 2005. Another file, grade-to-number.csv, gives the rules used by the institution in converting letter grades to numbers. The data files are part of a relational data base, a very important way of managing large amounts of data used by private and public institutions, corporations and governments — it’s the basis for a multi-billion dollar segment of the economy. Ordinarily, relational data bases are queried using special-purpose computer languages that sort, extract, and combine the data. Here are the R commands for converting the letter grades to numbers and extracting the data for one student: Grades = read.csv(&quot;http://tiny.cc/mosaic/grades.csv&quot;) gp = read.csv(&quot;http://tiny.cc/mosaic/grade-to-number.csv&quot;) all.students = merge(Grades, gp) one.student = subset( all.students, sid==&quot;S31509&quot; ) one.student ## grade sid sessionID gradepoint ## 192 A S31509 session3443 4.00 ## 543 A S31509 session2308 4.00 ## 674 A S31509 session2851 4.00 ## 1280 A S31509 session2737 4.00 ## 1389 A S31509 session2585 4.00 ## 1932 A- S31509 session2213 3.66 ## 2666 A- S31509 session2562 3.66 ## 4427 B+ S31509 session1959 3.33 ## 4438 B+ S31509 session3036 3.33 ## 5101 B+ S31509 session2928 3.33 ## 5387 C+ S31509 session2344 2.33 ## 5679 S S31509 session2764 NA ## 5902 S S31509 session2493 NA Calculating the mean grade-point for the one student is a simple matter: mean( ~ gradepoint, data=one.student ) ## [1] NA It’s equally straightforward to calculate the grade-point averages for all students as individuals: GPAs &lt;- mean( gradepoint ~ sid, data=all.students ) head(GPAs) ## S31185 S31188 S31191 S31194 S31197 S31200 ## 2.412500 NA NA 3.359167 3.356154 2.186429 Bootstrapping can be used to find the confidence interval on the grade-point average for each student: trials = do(100)*mean( ~ gradepoint, data=resample(one.student), na.rm = TRUE ) The na.rm = TRUE argument tells the mean function to exclude sampled grades of NA which are incompletes, withdrawls and audits on the student’s record. confint(trials) ## name lower upper level method estimate ## 1 mean 3.287671 3.824998 0.95 percentile 3.603636 It’s important to point out that there are other methods for calculating confidence intervals that are based on the standard deviation of the data. Formulas and procedures for such methods are given in just about every standard introductory statistics book and would certainly be used instead of bootstrapping in a simple calculation of the sort illustrated here. However, such formulas don’t go to the heart of the problem: accounting for variation in the grades and the contribution from different sources of that variation. For example, some of the variation in this student’s grades might be due systematically to improvement over time or due to differences between instructor’s practices. The modeling techniques introduced in the following chapters provide a means to examine and quantify the different sources of variation. "],
["language-of-models.html", "Chapter 6 Language of models 6.1 Bi-variate Plots 6.2 Fitting Models and Finding Model Values", " Chapter 6 Language of models At the core of the language of modeling is the notation that uses the tilde character (~) to identify the response variable and the explanatory variables. This notation is incorporated into many of operators that you will use. As usual, many of the operators, as well as the datasets required in this section come from the mosaic package, so it should be loaded if it isn’t loaded already. require(mosaic) To illustrate the computer commands for modeling and graphically displaying relationships between variables, use the utilities data set: Utils &lt;- Utilities # from mosaicData Some of the following examples make particular use of these variables #. ccf — the natural gas usage in cubic feet during the billing period. #. month — the month coded as 1 to 12 for January to December. #. temp — the average temperature during the billing period. Another illustrative example uses Current Population Survey wage data:`CPS` CPS &lt;- CPS85 # from mosaicData and focuses on the variables wage, sex, and sector. 6.1 Bi-variate Plots The basic idea of a bi-variate (two variable) plot is to examine one variable as it relates to another. The conventional format is to plot the response variable on the vertical axis and an explanatory variable on the horizontal axis. 6.1.1 Quantitative Explanatory Variable When the explanatory variable is quantitative, a scatter-plot is an appropriate graphical format. In the scatter plot, each case is a single point. The basic computer operator for making scatter plots is xyplot(): xyplot( ccf ~ temp, data = Utils) The first argument is a model formula written using the tilde modeling notation. This formula, ccf ~ temp is pronounced “ccf versus temperature.” It is traditional to plot the so-called dependent variable, here ccf, on the y–axis. In order to keep the model notation concise, the model formula has left out the name of the data frame to which the variables belong. Instead, the frame is specified in the data = argument. Since data has been set to be Utils, the formula ccf ~ temp is effectively translated to Utils$ccf ~ Utils$temp. You can specify the axis labels by hand, if you like. For example, xyplot( ccf ~ temp, data=Utils, xlab = &quot;Temperature (deg F)&quot;, ylab = &quot;Natural Gas Usage (ccf)&quot;) 6.1.2 Categorical Explanatory Variable When the explanatory variable is categorical, an appropriate format of display is the box-and-whiskers plot, made with the bwplot operator. Here, for example, is the wage versus sex from the Current Population Survey: bwplot( wage ~ sex, data=CPS) Notice that the outliers are setting the overall vertical scale for the graph and obscuring the detail at typical wage levels. You can use the ylim argument to set the scale of the y-axis however you want. For example: bwplot(wage~sector, data=CPS, ylim=c(0,30) ) You can also make side-by-side density plots which show more detail than the box-and-whisker plots. For instance: densityplot( ~ wage, groups = sex, data = CPS, auto.key = TRUE ) 6.1.3 Multiple Explanatory Variables The two-dimensional nature of paper or the computer screen lends itself well to displaying two variables: a response versus a single explanatory variable. Sometimes it is important to be able to add an additional explanatory variable. The graphics system gives a variety of options in this regard: Coding the additional explanatory variable] using color or symbol shapes. This is done by using the groups argument set to the name of the additional explanatory variable. For example: xyplot( wage ~ age, groups = sex, data = CPS, auto.key = TRUE) Splitting the plot into the groups defined by the additional explanatory variable. This is done by including the additional variable in the model formula using a | separator. For example: xyplot( wage ~ age | sex, data = CPS) 6.2 Fitting Models and Finding Model Values The lm operator (short for “Linear Model”) will translate a model design into fitted model values. It does this by “fitting” the model to data, a process that will be explained in later chapters. For now, focus on how to use lm to compute the fitted model values. The lm operator uses the same model language as in the book. To illustrate, consider the world-record swim-times data : Swim &lt;- read.csv(&quot;http://tiny.cc/mosaic/swim100m.csv&quot;) To construct the model time ~ 1 for the swim data: mod1 &lt;- lm( time ~ 1, data = Swim) Here the model has been given a name, mod1, so that you can refer to it later. You can use any name you like, so long as it is valid in R. Once the model has been constructed, the fitted values can be found using the fitted operator: fitted(mod1) ## 1 2 3 4 5 6 7 8 9 10 11 ## 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 ## 12 13 14 15 16 17 18 19 20 21 22 ## 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 ## 23 24 25 26 27 28 29 30 31 32 33 ## 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 ## 34 35 36 37 38 39 40 41 42 43 44 ## 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 ## 45 46 47 48 49 50 51 52 53 54 55 ## 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 ## 56 57 58 59 60 61 62 ## 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 59.92419 There is an individual fitted model value for each case. Of course, in this model all the model values are exactly the same since the model time ~ 1 treats all the cases as exactly the same. In later chapters you’ll see how to analyze the model values, make predictions from the model, and assess the contribution of each model term. For now, just look at the model values by plotting them out along with the data used. Plot out both the data values and the model values versus year just to emphasize that the model values are the same for every case: xyplot( time + fitted(mod1) ~ year, data = Swim) Pay careful attention to the syntax used in the above command. There are two quantities to the left of the ~. This is not part of the modeling language, where there is always a single response variable. Instead, it is a kind of shorthand, telling xyplot that it should plot out both of the quantities on the left side against the quantity on the right side. Of course, if you wanted to plot just the model values, without the actual data, you could specify the formula as fitted(mod1) ~ year. Here are more interesting models: mod2 &lt;- lm( time ~ 1 + year, data = Swim) mod3 &lt;- lm( time ~ 1 + sex, data = Swim) mod4 &lt;- lm( time ~ 1 + sex + year, data = Swim) mod5 &lt;- lm( time ~ 1 + year + sex + year:sex, data = Swim) You can, if you like, compare the fitted values from different models on one plot: xyplot( fitted(mod5) + fitted(mod3) ~ year, data = Swim, auto.key = TRUE) 6.2.1 Interactions and Main Effects Typically a model that includes an interaction term between two variables will include the main terms from those variables too. As a shorthand for this, the modeling language has a * symbol. So, the formula time ~ year + sex + year:sex can also be written time ~ year * sex. 6.2.2 Transformation Terms Transformation terms such as squares can also be included in the model formula. To mark the quantity clearly as a single term, it’s best to wrap the term with I() as follows: mod7 &lt;- lm( time ~ year + I(year^2) + sex, data = Swim) Another way to accomplish this, for polynomials, is to use the operator poly as in the model formula time ~ poly(year,2) + sex. Here’s a plot of the result: xyplot( time + fitted(mod7) ~ year, data = Swim) "],
["model-formulas-and-coefficients.html", "Chapter 7 Model formulas and coefficients 7.1 Examining model coefficients 7.2 Other Useful Operators", " Chapter 7 Model formulas and coefficients Make sure mosaic is loaded. 7.1 Examining model coefficients The lm() operator finds model coefficients. To illustrate, here’s a pair of statements that read in a data frame and fit a model to it: mod &lt;- lm( time ~ year + sex, data = SwimRecords) The first argument to lm is a model design, the second is the data frame. The object created by lm — here given the name mod — contains a variety of information about the model. To access the coefficients themselves, use the coef() operator applied to the model: coef(mod) ## (Intercept) year sexM ## 555.7167834 -0.2514637 -9.7979615 As shorthand to display the coefficients, just type the name of the object that is storing the model: mod ## ## Call: ## lm(formula = time ~ year + sex, data = SwimRecords) ## ## Coefficients: ## (Intercept) year sexM ## 555.7168 -0.2515 -9.7980 A more detailed report can be gotten with the summary operator. This gives additional statistical information that will be used in later chapters: summary(mod) ## ## Call: ## lm(formula = time ~ year + sex, data = SwimRecords) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.7027 -2.7027 -0.5968 1.2796 19.0759 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 555.71678 33.79991 16.441 &lt; 2e-16 *** ## year -0.25146 0.01732 -14.516 &lt; 2e-16 *** ## sexM -9.79796 1.01287 -9.673 8.79e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.983 on 59 degrees of freedom ## Multiple R-squared: 0.844, Adjusted R-squared: 0.8387 ## F-statistic: 159.6 on 2 and 59 DF, p-value: &lt; 2.2e-16 From time to time in the exercises, you will be asked to calculate model values “by hand.” This is accomplished by multiplying the coefficients by the appropriate values and adding them up. For example, the model value for a male swimmer in 2010 would be: 555.7 - 0.2515*2010 - 9.798 ## [1] 40.387 Notice that the “value” used to multiply the intercept is always 1, and the “value” used for a categorical level is either 0 or 1 depending on whether there is a match with the level. In this example, since the swimmer in question was male, the value of sex M is 1. If the swimmer had been female, the value for sex M would have been 0. When a model includes interaction terms, the interaction coefficients need to be multiplied by all the values involved in the interaction. For example, here is a model with an interaction between year and sex: mod2 &lt;- lm( time ~ year * sex, data = SwimRecords) coef(mod2) ## (Intercept) year sexM year:sexM ## 697.3012156 -0.3240459 -302.4638388 0.1499166 697.3 - 0.3240*2010 - 302.5 + 0.1499*2010 ## [1] 44.859 The year:sexM coefficient is being multiplied by the year (2010) and the value of sex M, which is 1 for this male swimmer. 7.2 Other Useful Operators cross() will combine two categorical variables into a single variable. For example, in the Current Population Survey data, the variable sex has levels F and M, while the variable race has levels W and NW. Crossing the two variables combines them; the new variable has four levels: F.NW, M.NW, F.W, M.W: CPS &lt;- CPS85 # from mosaicData RaceSex &lt;- cross(CPS$sex, CPS$race) summary(RaceSex) ## F:NW F:W M:NW M:W ## 28 217 39 250 The summary tells us that there are 28 non-white females, 270 white females, etc, in the new categorical variable called RaceSex. as.factor() will convert a quantitative variable to a categorical variable. This is useful when a quantity like month has been coded as a number, say 1 for January and 2 for February, etc. but you do not want models to treat it as such. To illustrate, consider two different models of the usage temperature versus month: utils &lt;- read.csv(&quot;http://tiny.cc/mosaic/utilities.csv&quot;) mod1 &lt;- lm( temp ~ month, data = utils) mod2 &lt;- lm( temp ~ as.factor(month), data = utils) Here are the graphs of those models: xyplot(temp + fitted(mod1) ~ month, data = utils) xyplot( temp + fitted(mod2) ~ month, data = utils ) In the first model, month is treated quantitatively, so the model term month produces a straight-line relationship that does not correspond well to the data. In the second model, month is treated categorically, allowing a more complicated model relationship. In fact, this is a groupwise model: the model values represent the mean temperature for each month. "],
["fitting-models-to-data.html", "Chapter 8 Fitting models to data 8.1 Sums of Squares 8.2 Redundancy", " Chapter 8 Fitting models to data Using the lm software is largely a matter of familiarity with the model design language described in Chapter @ref(“chap:language”). Computing the fitted model values and the residuals is done with the fitted and resid. These operators take a model as an input. To illustrate: Swim &lt;- SwimRecords # from mosaicData mod1 &lt;- lm( time ~ year + sex, data = Swim) coef(mod1) ## (Intercept) year sexM ## 555.7167834 -0.2514637 -9.7979615 Once you have constucted the model, you can use fitted and resid: modvals &lt;- fitted(mod1) head(modvals) ## 1 2 3 4 5 6 ## 66.88051 66.12612 65.62319 65.12026 63.61148 63.10855 8.1 Sums of Squares Computations can be performed on the fitted model values and the residuals, just like any other quantity: mean(fitted(mod1)) ## [1] 59.92419 var(resid(mod1)) ## [1] 15.34147 sd(resid(mod1)) ## [1] 3.916818 summary(resid(mod1)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -4.7030 -2.7030 -0.5968 0.0000 1.2800 19.0800 Sums of squares are very important in statistics. Here’s how to calculate them for the response values, the fitted model values, and the residuals: sum(Swim$time^2) ## [1] 228635 sum(fitted(mod1)^2) ## [1] 227699.2 sum(resid(mod1)^2) ## [1] 935.8294 The partitioning of variation by models is seen by the way the sum of squares of the fitted and the residuals add up to the sum of squares of the response: 227699 + 935.8 ## [1] 228634.8 Don’t forget the squaring stage of the operation! The sum of the residuals (without squaring) is very different from the sum of squares of the residuals: sum(resid(mod1)) ## [1] 1.848521e-14 sum(resid(mod1)^2) ## [1] 935.8294 Take care in reading numbers formatted like 1.849e-14. The notation stands for \\(1.849 \\times 10^{-14}\\). That number, \\(0.00000000000001849\\), is effectively zero compared to the residuals themselves! 8.2 Redundancy The lm operator will automatically detect redundancy and deal with it by leaving the redundant terms out of the model. To see how redundancy is handled, here is an example with a constructed redundant variable in the swimming dataset. The following statement adds a new variable to the dataframe counting how many years after the end of World War II each record was established: Swim$afterwar &lt;- Swim$year - 1945 Here is a model that doesn’t involve redundancy mod1 &lt;- lm( time ~ year + sex, data = Swim) coef(mod1) ## (Intercept) year sexM ## 555.7167834 -0.2514637 -9.7979615 When the redundant variable is added in, lm successfully detects the redundancy and handles it. This is indicated by a coefficient of NA on the redundant variable. mod2 &lt;- lm( time ~ year + sex + afterwar, data = Swim) coef(mod2) ## (Intercept) year sexM afterwar ## 555.7167834 -0.2514637 -9.7979615 NA In the absence of redundancy, the model coefficients don’t depend on the order in which the model terms are specified. But this is not the case when there is redundancy, since any redundancy is blamed on the later variables. For instance, here afterwar has been put first in the explanatory terms, so lm identifies year as the redundant variable: mod3 &lt;- lm( time ~ afterwar + year + sex, data = Swim) coef(mod3) ## (Intercept) afterwar year sexM ## 66.6199228 -0.2514637 NA -9.7979615 Even though the coefficients are different, the fitted model values and the residuals are exactly the same (to within computer round-off) regardless of the order of the model terms. head(fitted(mod2)) ## 1 2 3 4 5 6 ## 66.88051 66.12612 65.62319 65.12026 63.61148 63.10855 head(fitted(mod3)) ## 1 2 3 4 5 6 ## 66.88051 66.12612 65.62319 65.12026 63.61148 63.10855 Note that whenever you use a categorical variable and an intercept term in a model, there is a redundancy. This is not shown explicitly. For example, here is a model with no intercept term, and both levels of the categorical variable sex show up with coefficients: mod &lt;- lm( time ~ sex - 1, data = Swim) coef(mod) ## sexF sexM ## 65.19226 54.65613 If the intercept term is included (as it is by default unless -1 is used in the model formula), one of the levels is simply dropped in the report: mod &lt;- lm( time ~ sex, data = Swim) coef(mod) ## (Intercept) sexM ## 65.19226 -10.53613 Remember that this coefficient report implicitly involves a redundancy. "],
["correlation-and-partitioning-of-variation.html", "Chapter 9 Correlation and partitioning of variation", " Chapter 9 Correlation and partitioning of variation The coefficient of determination, \\(R^2\\), compares the variation in the response variable to the variation in the fitted model value. It can be calculated as a ratio of variances: Swim &lt;- SwimRecords # from mosaicData mod &lt;- lm( time ~ year + sex, data = Swim) var(fitted(mod)) / var(Swim$time) ## [1] 0.8439936 The convenience function rsquared() does the calculation for you: rsquared(mod) ## [1] 0.8439936 The regression report is a standard way of summarizing models. Such a report is produced by most statistical software packages and used in many fields. The first part of the table contains the coefficients — labeled “Estimate” — along with other information that will be introduced starting in Chapter @ref(“chap:confidence”). The \\(R^2\\) statistic is a standard part of the report; look at the second line from the bottom. summary(mod) ## ## Call: ## lm(formula = time ~ year + sex, data = Swim) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.7027 -2.7027 -0.5968 1.2796 19.0759 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 555.71678 33.79991 16.441 &lt; 2e-16 *** ## year -0.25146 0.01732 -14.516 &lt; 2e-16 *** ## sexM -9.79796 1.01287 -9.673 8.79e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.983 on 59 degrees of freedom ## Multiple R-squared: 0.844, Adjusted R-squared: 0.8387 ## F-statistic: 159.6 on 2 and 59 DF, p-value: &lt; 2.2e-16 Occasionally, you may be interested in the correlation coefficient \\(r\\) between two quantities. You can, of course, compute \\(r\\) by fitting a model, finding \\(R^2\\), and taking a square root. mod2 &lt;- lm( time ~ year, data = Swim) coef(mod2) ## (Intercept) year ## 567.2420024 -0.2598771 sqrt(rsquared(mod2)) ## [1] 0.7723752 The cor() function computes this directly: cor(Swim$time, Swim$year) ## [1] -0.7723752 Note that the negative sign on \\(r\\) indicates that record swim time decreases as year increases. This information about the direction of change is contained in the sign of the coefficient from the model. The magnitude of the coefficient tells how fast the time is changing (with units of seconds per year). The correlation coefficient (like \\(R^2\\)) is without units. Keep in mind that the correlation coefficient \\(r\\) summarizes only the simple linear model A ~ B where B is quantitative. But the coefficient of determination, \\(R^2\\), summarizes any model; it is much more useful. If you want to see the direction of change, look at the sign of the correlation coefficient. "],
["total-and-partial-relationships.html", "Chapter 10 Total and partial relationships 10.1 Adjustment", " Chapter 10 Total and partial relationships 10.1 Adjustment There are two basic approaches to adjusting for covariates. Conceptually, the simplest one is to hold the covariates constant at some level when collecting data or by extracting a subset of data which holds those covariates constant. The other approach is to include the covariates in your models. For example, suppose you want to study the differences in the wages of male and females. The very simple model wage ~ sex might give some insight, but it attributes to sex effects that might actually be due to level of education, age, or the sector of the economy in which the person works. Here’s the result from the simple model: Cps &lt;- CPS85 mod0 &lt;- lm( wage ~ sex, data = Cps) summary(mod0) ## ## Call: ## lm(formula = wage ~ sex, data = Cps) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.995 -3.529 -1.072 2.394 36.621 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.8789 0.3216 24.50 &lt; 2e-16 *** ## sexM 2.1161 0.4372 4.84 1.7e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.034 on 532 degrees of freedom ## Multiple R-squared: 0.04218, Adjusted R-squared: 0.04038 ## F-statistic: 23.43 on 1 and 532 DF, p-value: 1.703e-06 The coefficients indicate that a typical male makes $2.12 more per hour than a typical female. (Notice that \\(R^2 = 0.0422\\) is very small: sex explains hardly any of the person-to-person variability in wage.) By including the variables age, educ, and sector in the model, you can adjust for these variables: mod1 &lt;- lm( wage ~ age + sex + educ + sector, data = Cps) summary(mod1) ## ## Call: ## lm(formula = wage ~ age + sex + educ + sector, data = Cps) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.198 -2.695 -0.465 2.066 35.159 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.69411 1.53776 -3.053 0.002384 ** ## age 0.10221 0.01657 6.167 1.39e-09 *** ## sexM 1.94172 0.42285 4.592 5.51e-06 *** ## educ 0.61556 0.09439 6.521 1.65e-10 *** ## sectorconst 1.43552 1.13120 1.269 0.204999 ## sectormanag 3.27105 0.76685 4.266 2.37e-05 *** ## sectormanuf 0.80627 0.73115 1.103 0.270644 ## sectorother 0.75838 0.75918 0.999 0.318286 ## sectorprof 2.24777 0.66976 3.356 0.000848 *** ## sectorsales -0.76706 0.84202 -0.911 0.362729 ## sectorservice -0.56871 0.66602 -0.854 0.393556 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.334 on 523 degrees of freedom ## Multiple R-squared: 0.3022, Adjusted R-squared: 0.2888 ## F-statistic: 22.65 on 10 and 523 DF, p-value: &lt; 2.2e-16 The adjusted difference between the sexes is $1.94 per hour. (The \\(R^2=0.30\\) from this model is considerably larger than for mod0, but still a lot of the person-to-person variation in wages has not be captured.) It would be wrong to claim that simply including a covariate in a model guarantees that an appropriate adjustment has been made. The effectiveness of the adjustment depends on whether the model design is appropriate, for instance whether appropriate interaction terms have been included. However, it’s certainly the case that if you don’t include the covariate in the model, you have not adjusted for it. The other approach is to subsample the data so that the levels of the covariates are approximately constant. For example, here is a subset that considers workers between the ages of 30 and 35 with between 10 to 12 years of education and working in the sales sector of the economy: small &lt;- subset(Cps, age &lt;=35 &amp; age &gt;= 30 &amp; educ&gt;=10 &amp; educ &lt;=12 &amp; sector==&quot;sales&quot; ) The choice of these particular levels of age, educ, and sector is arbitrary, but you need to choose some level if you want to hold the covariates appproximately constant. The subset of the data can be used to fit a simple model: mod4 &lt;- lm( wage ~ sex, data = small) summary(mod4) ## ## Call: ## lm(formula = wage ~ sex, data = small) ## ## Residuals: ## 10 156 195 ## 0.5 0.0 -0.5 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.500 0.500 9.000 0.0704 . ## sexM 4.500 0.866 5.196 0.1210 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7071 on 1 degrees of freedom ## Multiple R-squared: 0.9643, Adjusted R-squared: 0.9286 ## F-statistic: 27 on 1 and 1 DF, p-value: 0.121 At first glance, there might seem to be nothing wrong with this approach and, indeed, for very large data sets it can be effective. In this case, however, there are only 3 cases that satisfy the various criteria: two women and one man. table( small$sex ) ## ## F M ## 2 1 So, the $4.50 difference between the sexes and wages depends entirely on the data from a single male! (Chapter @ref(“chap:confidence”) describes how to assess the precision of model coefficients. This one works out to be \\(4.50 \\pm 11.00\\) — not at all precise.) "],
["modeling-randomness.html", "Chapter 11 Modeling randomness 11.1 Random Draws from Probability Models 11.2 Standard Probability Models 11.3 Quantiles and Coverage Intervals 11.4 Percentiles", " Chapter 11 Modeling randomness One of the most surprising outcomes of the revolution in computing technology has been the discovery of diverse uses for randomness in the analysis of data and in science generally. Most young people have little trouble with the idea of a computer generating random data; they see it in computer games and simulations. Older people, raised with the idea that computers do mathematical operations and that such operations are completely deterministic, sometimes find computer-generated randomness suspect. Indeed, conventional algebraic notation (\\(+\\), \\(-\\), \\(\\sqrt{ }\\), \\(\\cos\\), and so on) has no notation for “generate at random.” One of the simplest operators for generating random events is resample. This takes two arguments: the first is a set of items to choose from at random, the second is how many events to generate. Each item is equally likely to be choosen. For example, here is a simulation of a coin flip: coin &lt;- c(&quot;H&quot;,&quot;T&quot;) resample(coin, 5) ## [1] &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; resample(coin, 5) ## [1] &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; The first command creates an object holding the possible outcome of each event, called coin. The next command generated five events, each event being a random choice of the outcomes in coin. Another example is rolling dice. First, construct a set of the possible outcomes: the numbers 1, 2, 3, 4, 5, 6. die &lt;- seq(1,6) die ## [1] 1 2 3 4 5 6 Then generate random events. Here is a roll of two dice. resample(die,2) ## [1] 3 5 The resample() function is also useful for selecting cases at random from a data frame. You have already seen it put to use in generating sampling distributions via the bootstrap. This technique will be further developed in later chapters. 11.1 Random Draws from Probability Models Although resample() is useful for random sampling, it can work only with finite sets of possible outcomes such as H/T or 1/2/3/4/5/6 or the cases in a data frame. By default in resample(), the underlying probability model is equiprobability — each possible outcome is equally likely. You can specify another probability model by using the prob = argument to resample(). For instance, to flip coins that are very likely (90% of the time, on average) to come up heads: resample( coin, 10, prob = c(.9,.1)) ## [1] &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; R provides other operators that allow draws to be made from outcome sets that are infinite. For example, the rnorm() function makes random draws from a normal probability distribution. The required argument tells how many draws to make. Optional, named arguments let you specify the mean and standard deviation of the particular normal distribution that you want. To illustrate, here is a set of 15 random numbers from a normal distribution with mean 1000 and standard deviation 75: samps &lt;- rnorm(15, mean = 1000, sd = 75) samps ## [1] 1171.4984 895.8354 979.0908 990.0009 1047.6963 978.6810 800.7658 816.9650 1099.0085 ## [10] 977.0021 866.4019 987.1062 1091.1006 1142.1395 967.7148 In this example, the output was assigned to an object samps to facilitate some additional computations to the values. For instance, here is the mean and standard deviation of the sample: mean(samps) ## [1] 987.4005 sd(samps) ## [1] 110.9166 Don’t be surprised that the mean and standard deviation of the sample don’t match exactly the parameters that were set with the arguments mean = 1000, sd = 75. The sample was drawn at random and so the sample statistics are going to vary from one sample to the next. Part of the statistical methodology to be studied in later chapters has to do with determining how close the statistics calculated from a sample are likely to be to the parameters of the underlying population. Often you will generate very large samples. In these situations you usually don’t want to display all the samples, just do calculations with them. The practical limits of “large” depend on the computer you are using and how much time you are willing to spend on a calculation. For an operator like rnorm and the others to be introduced in this chapter, it’s feasible to generate samples of size 10,000 or 100,000 on an ordinary laptop computer. samps &lt;- rnorm(100000, mean = 1000, sd = 75) mean( samps ) ## [1] 999.6824 sd( samps ) ## [1] 75.23447 Notice that the sample mean and standard deviation are quite close to the population parameters in this large sample. (Remember not to put commas in as punctuation in large numbers: it’s 100000 not 100,000.) The simulations that you will do in later chapters will be much more elaborate than the simple draws here. Even with today’s computers, you will want to use only a few hundred trials. 11.2 Standard Probability Models R provides a large set of operators like rnorm for different probability models. All of these operators work in the same way: Each has a required first argument that gives the number of draws to make. Each has an optional set of parameters that specify the particular probability distribution you want. All the operators start with the letter r — standing for “random” — followed by the name of the probability model: Family R name Parameters Nature Normal rnorm mean,sd continuous Uniform runif min,max continuous Binomial rbinom size,prob discrete Poisson rpois Average rate (written lambda) discrete Exponential rexp Same rate as in poisson but the parameter is called rate. continuous Lognormal rlnorm Mean and sd of the natural logarithm. meanlog, sdlog continuous \\(\\chi^2\\) rchisq Degrees of freedom (df) continuous t rt Degrees of freedom (df) continuous F rf Degrees of freedom in the numerator and in the denominator (df1, df2) continuous To use these operators, you first must choose a particular probability model based on the setting that applies in your situation. This setting will usually indicate what the population parameters should be. Some examples: You are in charge of a hiring committee that is going to interview three candidates selected from a population of job applicants that is 63% female. How many of the interviewees will be female? Modeling this as random selection from the applicant pool, a binomial model is appropriate. The size of each trial is 3, the probability of being female is 63% : samps &lt;- rbinom(40, size = 3, prob = 0.63) samps ## [1] 2 2 2 3 2 2 3 2 1 1 2 1 3 2 2 2 2 1 2 2 1 3 2 3 2 2 3 3 1 3 2 2 2 1 1 2 1 3 1 2 There are 40 trials here, since the first argument was set to 40. Remember, each of the trials is a simulation of one hiring event. In the first simulated event, two of the interviewees were female; in the third only one was female. Typically, you will be summarizing all the simulations, for example to see how likely each possible outcome is. table(samps) ## samps ## 1 2 3 ## 10 21 9 You want to simulate the number of customers who come into a store over the course of an hour. The average rate is 15 per hour. To simulate a situation where customers arrive randomly, the poisson model is appropriate: rpois(25, lambda = 15) ## [1] 17 15 19 25 10 17 15 12 11 15 19 17 13 13 13 12 18 14 21 17 15 11 17 18 11 You want to generate a simulation of the interval between earthquakes as in Example @ref(“example:earthquake-intervals”). To simulate the random intervals with a typical rate of 0.03 earthquakes per year, you would use rexp( 15, rate = 0.03 ) ## [1] 49.5057149 37.6415609 177.9802139 0.1406587 29.8780978 42.6115631 0.1706456 40.2302083 ## [9] 45.6183367 210.6326415 63.0320137 35.7448324 2.7317871 16.6463322 89.4471805 Notice the huge variation in the intervals, from less than two months to more than 210 years between earthquakes. 11.3 Quantiles and Coverage Intervals You will often need to compute coverage intervals in order to describe the range of likely outcomes from a random process. R provides a series of operators for this purpose; a separate operator for each named probability model. The operators all begin with q, standing for quantiles. In all cases, the first argument is the set of quantiles you want to calculate for the particular probability model. The optional named arguments are the parameters. Remember that to find a 95% coverage interval you need the 0.025 and 0.975 quantiles. For a 99% interval, you need the 0.005 and 0.995 quantiles. To illustrate, here are 95% coverage intervals for a few probability models. A normal distribution with mean 0 and standard deviation 1: qnorm( c(0.025, 0.975), mean = 0, sd = 1) ## [1] -1.959964 1.959964 The hiring committee situation modelled by a binomial distribution with size = 3 and prob = 0.63: qbinom( c(0.025, 0.975), size = 3, prob = 0.63) ## [1] 0 3 Perhaps you are surprised to see that the coverage interval includes all the possible outcomes. That’s because the number of cases in each trial (\\(n = 3\\)) is quite small. The number of customers entering a store during an hour as modelled by a poisson distribution with an average rate of 15 per hour. qpois( c(0.025, 0.975), lambda = 15) ## [1] 8 23 The interval between earthquakes modelled by an exponential distribution with a typical rate of 0.03 earthquakes per year: qexp( c(.025, .975), rate = 0.03) ## [1] 0.8439269 122.9626485 You can also use the q operators to find the value that would be at a particular percentile. For example, the exponential model with rate = 0.03 gives the 25th percentile of the interval between earthquakes as: qexp( .25, rate = 0.03) ## [1] 9.589402 A quarter of the time, the interval between earthquakes will be 9.59 years or less. It’s entirely feasible to calculate percentiles and coverage intervals by combining the random-number generators with quantile. For example, here is the 95% coverage interval from a normal distribution with mean 0 and standard deviation 1: samps &lt;- rnorm(10000, mean = 0, sd = 1) qdata( samps, c(.025, .975) ) ## quantile p ## 2.5% -1.961533 0.025 ## 97.5% 1.892216 0.975 The disadvantage of this approach is that it is a simulation and the results will vary randomly. By making the sample size large enough — here it is \\(n = 10000\\) — you can reduce the random variation. Using the q operators uses mathematical analysis to give you what is effectively an infinite sample size. For this reason, it’s advisable to use the q operators when you can. However, for many of the techniques to be introduced in later chapters you will have to generate a random sample and then apply quantile to approximate the coverage intervals. 11.4 Percentiles A percentile computation applies to situations where you have a measured value and you want to know where that value ranks relative to the entire set of possible outcomes. You have already seen percentiles computed from samples; they also apply to probability models. It’s easy to confuse percentiles with quantiles because they are so closely related. Mathematically, the percentile operators are the inverse of the quantile operators. To help you remember which is which, it’s helpful to distinguish them based on the type of argument that you give to the operator: Percentile (e.g. pnorm) The input argument is a measured value, something that could be the output of a single draw from the probability distribution. The output is always a number between 0 and 1 — a percentile. Quantile (e.g. qnorm) The input is a percentile, a number between 0 and 1. The output is on the scale of the measured variable. Example: You have just gotten your score, 670, on a professional school admissions test. According to the information published by the testing company, the scores are normally distributed with a mean of 600 and a standard deviation of 100. So, your ranking on the test, as indicated by a percentile, is: pnorm(670, mean = 600, sd = 100) ## [1] 0.7580363 Your score is at about the 75th percentile. Example: Unfortunately, the professional school that you want to go to accepts only students with scores in the top 15 percent. Your score, at 75.8%, isn’t good enough. So, you will study some more and take practice tests until your score is good enough. How well will you need to score to reach the 85th percentile? qnorm(0.85, mean = 600, sd = 100) ## [1] 703.6433 "],
["confidence-in-models.html", "Chapter 12 Confidence in models 12.1 Confidence Intervals from Standard Errors 12.2 Bootstrapping Confidence Intervals 12.3 Prediction Confidence Intervals", " Chapter 12 Confidence in models Regression reports are generated using software you have already encountered: lm to fit a model and summary to construct the report from the fitted model. To illustrate with the SwimRecords data from the mosaicData package: mod &lt;- lm(time ~ year + sex, data = SwimRecords) summary(mod) ## ## Call: ## lm(formula = time ~ year + sex, data = SwimRecords) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.7027 -2.7027 -0.5968 1.2796 19.0759 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 555.71678 33.79991 16.441 &lt; 2e-16 *** ## year -0.25146 0.01732 -14.516 &lt; 2e-16 *** ## sexM -9.79796 1.01287 -9.673 8.79e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.983 on 59 degrees of freedom ## Multiple R-squared: 0.844, Adjusted R-squared: 0.8387 ## F-statistic: 159.6 on 2 and 59 DF, p-value: &lt; 2.2e-16 12.1 Confidence Intervals from Standard Errors Given the coefficient estimate and the standard error from the regression report, the confidence interval is easily generated. For a 95% confidence interval, you just multiply the standard error by 2 to get the margin of error. For example, in the above, the margin of error on sex M is \\(2 \\times 1.013 = 2.03\\), or, in computer notation: 2 * 1.0129 ## [1] 2.0258 If you want the two endpoints of the confidence interval, rather than just the margin of error, do these simple calculations: (1) subtract the margin of error from the estimate; (2) add the margin of error to the estimate. So, -9.798 - 2*1.0129 ## [1] -11.8238 -9.798 + 2*1.0129 ## [1] -7.7722 The key thing is to remember the multiplier that is applied to the standard error. A multiplier of approximately 2 is for a 95% confidence interval. The confint() function provides a convenient way to calculate confidence intervals directly. It calculates the exact multiplier (which depends somewhat on the sample size) and applies it to the standard error to produce the confidence intervals. mod &lt;- lm( time ~ year + sex, data = SwimRecords) confint(mod) ## 2.5 % 97.5 % ## (Intercept) 488.0833106 623.3502562 ## year -0.2861277 -0.2167997 ## sexM -11.8247135 -7.7712094 It would be convenient if the regression report included confidence intervals rather than the standard error. Part of the reason it doesn’t is historical: the desire to connect to the traditional by-hand calculations. 12.2 Bootstrapping Confidence Intervals Confidence intervals on model coefficients can be computed using the same bootstrapping technique introduced in Chapter @ref(“chap:statistical-inference”). Start with your fitted model. To illustrate, here is a model of world-record swimming time over the years, taking into account sex: lm( time ~ year + sex, data = SwimRecords) ## ## Call: ## lm(formula = time ~ year + sex, data = SwimRecords) ## ## Coefficients: ## (Intercept) year sexM ## 555.7168 -0.2515 -9.7980 These coefficients reflect one hypothetical draw from the population-based sampling distribution. It’s impossible to get another draw from the “population” here: the actual records are all you’ve got. But to approximate sampling variation, you can treat the sample as your population and re-sample: lm( time ~ year + sex, data = resample(SwimRecords)) ## ## Call: ## lm(formula = time ~ year + sex, data = resample(SwimRecords)) ## ## Coefficients: ## (Intercept) year sexM ## 533.8004 -0.2402 -9.9231 Constructing many such re-sampling trials and collect the results s = do(500) * lm( time ~ year + sex, data = resample(SwimRecords)) head(s) ## Intercept year sexM sigma r.squared F numdf dendf .row .index ## 1 551.9951 -0.2495943 -9.495649 2.841716 0.9078873 290.7598 2 59 1 1 ## 2 555.8467 -0.2513771 -10.600461 4.007877 0.8340924 148.3099 2 59 1 2 ## 3 693.8656 -0.3217095 -9.636199 5.493706 0.8373505 151.8716 2 59 1 3 ## 4 633.5587 -0.2911209 -9.212951 4.760334 0.8182637 132.8231 2 59 1 4 ## 5 588.4652 -0.2683256 -9.557587 3.912234 0.8497676 166.8624 2 59 1 5 ## 6 544.5205 -0.2460383 -9.379971 3.127280 0.8878750 233.5992 2 59 1 6 To find the standard error of the coefficients, just take the standard deviation across the re-sampling trials. For the indicator variable sex M: sd(s$sexM) ## [1] 0.9242203 Multiplying the standard error by 2 gives the approximate 95% margin of error. Alternatively, you can use the confint() function to calculate this for you: confint(s$sexM, method = &quot;stderr&quot;) ## Confidence Interval from Bootstrap Distribution (500 replicates) ## V1 V2 ## stderr -11.5434 -7.911714 12.3 Prediction Confidence Intervals When a model is used to make a prediction, it’s helpful to be able to describe how precise the prediction is. For instance, suppose you want to use the KidsFeet data set (from mosaicData) to make a prediction of the foot width of a girl whose foot length is 25 cm. First, fit your model: names(KidsFeet) ## [1] &quot;name&quot; &quot;birthmonth&quot; &quot;birthyear&quot; &quot;length&quot; &quot;width&quot; &quot;sex&quot; &quot;biggerfoot&quot; ## [8] &quot;domhand&quot; levels(KidsFeet$sex) ## [1] &quot;B&quot; &quot;G&quot; mod &lt;- lm( width ~ length + sex, data = KidsFeet) Now apply the model to the new data for which you want to make a prediction. Take care to use the right coding for categorical variables. predict(mod, newdata = data.frame( length=25, sex=&quot;G&quot; )) ## 1 ## 8.934275 In order to generate a confidence interval, the predict operator needs to be told what type of interval is wanted. There are two types of prediction confidence intervals: Interval on the model value which reflects the sampling distributions of the coefficients themselves. To calculate this, use the interval = &quot;confidence&quot; named argument: predict(mod, newdata = data.frame( length = 25, sex = &quot;G&quot; ), interval = &quot;confidence&quot;) ## fit lwr upr ## 1 8.934275 8.742565 9.125985 The components named lwr and upr are the lower and upper limits of the confidence interval, respectively. Interval on the prediction which includes the variation due to the uncertainty in the coefficients as well as the size of a typical residual. To find this interval, use the interval = &quot;prediction&quot; named argument: predict(mod, newdata = data.frame( length = 25, sex = &quot;G&quot; ), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 8.934275 8.130484 9.738066 The prediction interval is larger than the model-value confidence interval because the residual always gives additional uncertainty around the model value. Predicting an individual’s foot width, even if we know her sex and foot length, involves a lot more uncertainty than predicting the mean foot width of all individuals with these particular characteristics. "],
["the-logic-of-hypothesis-testing.html", "Chapter 13 The logic of hypothesis testing", " Chapter 13 The logic of hypothesis testing "],
["testing-whole-models.html", "Chapter 14 Testing whole models 14.1 The Permutation Test 14.2 First-Principle Tests", " Chapter 14 Testing whole models require(mosaic) # mosaic operators and data will be used in this section The chapter presents two ways of doing hypothesis tests on whole models: (1) permutation tests where the connection is severed between the explanatory and response variables, (2) tests such as ANOVA where the sampling distribution is calculated from first principles. In practice, first-principle tests are used most of the time. Still, the permutation test is useful for developing intuition about hypothesis testing — our main purpose here — and for those not-so-rare occasions where the assumptions behind the first-principle tests are dubious. 14.1 The Permutation Test The idea of a permutation test is to enforce the null hypothesis that there is no connection between the response variables and the explanatory variables. An effective way to do this is to randomize the response variable in a way that is consistent with sampling variability. When constructing confidence intervals, the resample() function was used. Re-sampling will typically repeat some cases and omit others. Here, the shuffle() function will be used instead, to scramble the order of one or more variables while leaving the others in their original state. To illustrate, consider a model for exploring whether sex and mother’s height are related to the height of the child: G &lt;- Galton # from mosaicData mod &lt;- lm( height ~ sex + mother, data = G) coefficients(mod) ## (Intercept) sexM mother ## 41.4495235 5.1766949 0.3531371 The coefficients indicate that typical males are taller than typical females by about 5 inches and that for each inch taller the mother is, a child will typically be taller by 0.35 inches. A reasonable test statistic to summarize the whole model is \\(R^2\\): rsquared(mod) ## [1] 0.5618019 For confidence intervals, re-sampling was applied to the entire data frame. This selects random cases, but each selected case is an authentic one that matches exactly the original values for that case. The point of re-sampling is to get an idea of the variability introduced by random sampling of authentic cases. do(5) * lm( height ~ sex + mother, data = resample(G)) ## Intercept sexM mother sigma r.squared F numdf dendf .row .index ## 1 42.62282 5.198677 0.3344904 2.357899 0.5689608 590.6887 2 895 1 1 ## 2 41.33571 5.167612 0.3528718 2.347953 0.5587133 566.5800 2 895 1 2 ## 3 40.63105 5.166878 0.3651324 2.310658 0.5821128 623.3632 2 895 1 3 ## 4 39.34922 5.110852 0.3869812 2.368210 0.5574939 563.7855 2 895 1 4 ## 5 37.85104 5.312496 0.4077655 2.416791 0.5631884 576.9692 2 895 1 5 The sex M coefficients are tightly grouped near 5 inches, the mother coefficients are around 0.3 to 0.4. In order to carry out a permutation test, do not randomize the whole data frame. Instead, shuffle just the response variable: do(5) * lm( shuffle(height) ~ sex + mother, data = G) ## Intercept sexM mother sigma r.squared F numdf dendf .row .index ## 1 66.43890 0.26931216 0.00284531 3.584389 0.0014106973 0.6321789 2 895 1 1 ## 2 65.95435 0.10753345 0.01171354 3.586428 0.0002739812 0.1226402 2 895 1 2 ## 3 68.23226 0.07942871 -0.02360484 3.586263 0.0003658552 0.1637801 2 895 1 3 ## 4 63.22838 -0.03380593 0.05539274 3.584576 0.0013063829 0.5853711 2 895 1 4 ## 5 67.93034 -0.15586484 -0.01699231 3.585887 0.0005757963 0.2578173 2 895 1 5 Now the sex M and mother coefficients are close to zero, as would be expected when there is no relationship between the response variable and the explanatory variables. In constructing the sampling distribution under the null hypothesis, you should do hundreds of trials of fitting the model to the scrambled data, calculating the test statistic (\\(R^2\\) here) for each trial. Note that each trial itself involves all of the cases in your sample, but those cases have been changed so that the shuffled variable almost certainly takes on a different value in every case than in the original data. nulltrials = do(500) * rsquared( lm(shuffle(height) ~ sex + mother, data = G)) Notice that the rsquared() operator has been used to calculate the test statistic \\(R^2\\) from the model. The output of do() is a data frame: head(nulltrials) ## rsquared ## 1 0.0022598018 ## 2 0.0003179839 ## 3 0.0010647410 ## 4 0.0050407000 ## 5 0.0047568723 ## 6 0.0006001192 Naturally, all of the \\(R^2\\) values for the trials are close to zero. After all, there is no relation between the response variable (after randomization with shuffle()) and the explanatory variables. The p-value can be calculated directly from the trials, by comparison to the observed value in the actual data: \\(R^2\\) was 0.5618. tally(nulltrials$rsquared &gt; 0.5618) ## ## TRUE FALSE ## 0 500 None of the 500 trials were greater than the value of the test statistic, 0.5618. It wouldn’t be fair to claim that \\(p=0\\), since we only did 500 trials, but it is reasonable to say that the permutation test shows the p-value is \\(p \\leq 1/500\\). For interst’s sake, have a look at the null distribution for \\(R^2\\): densityplot(nulltrials$rsquared) These values are a long way from 0.5618. The p-value is likely to be much less than \\(1/500\\). 14.2 First-Principle Tests On modern computers, the permutation test is entirely practical. But a few decades ago, it was not. Great creativity was applied to finding test statistics where the sampling distribution could be estimated without extensive calculation. One of these is the F statistic. This is still very useful today and is a standard part of the regression report in many statistical packages. Here is the regression report from the height ~ sex+mother: mod &lt;- lm( height ~ sex + mother, data = G) summary(mod) ## ## Call: ## lm(formula = height ~ sex + mother, data = G) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.4036 -1.6024 0.1528 1.5890 9.4199 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 41.44952 2.20949 18.76 &lt;2e-16 *** ## sexM 5.17669 0.15867 32.62 &lt;2e-16 *** ## mother 0.35314 0.03439 10.27 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.374 on 895 degrees of freedom ## Multiple R-squared: 0.5618, Adjusted R-squared: 0.5608 ## F-statistic: 573.7 on 2 and 895 DF, p-value: &lt; 2.2e-16 The last line of the report shows an F statistic of 574 based on an \\(R^2\\) of 0.562 and translates this to a p-value that is practically zero: &lt;2e-16. By way of showing that the regression report is rooted in the same approach shown in the chapter, you can confirm the calculations. There are \\(m=3\\) coefficients and \\(n=898\\) cases, producing \\(n-m=895\\) degrees of freedom in the denominator and \\(m-1=2\\) degrees of freedom in the numerator. The calculation of the F statistic from \\(R^2\\) and the degrees of freedom follows the formula given in the chapter. \\[ F = \\frac{\\frac{R^2}{m-1}}{\\frac{1-R^2} {n-m}} \\] Plugging the values into the formula (0.562 / 2) / ((1-.562) / 895) ## [1] 574.1895 F is the test statistic. To convert it to a p-value, you need to calculate how extreme the value of F\\(=574.2\\) is with reference to the F distribution with 895 and 2 degrees of freedom. 1 - pf( 574.2, 2, 895) ## [1] 0 The calculation of p-values from F always follows this form. In the context of the F distribution, “extreme” always means “bigger than.” So, calculate the area under the F distribution to the right of the observed value. Here’s a picture of the relevant F distribution: f &lt;- rf(1000, 2, 895) # 1000 values from the f distribution densityplot( f, xlim = c(0,10), main=&quot;density plot for F(2,895)&quot; ) Very little of this distribution lies to the right of 6; virtually none of it lies to the right of 574. "],
["testing-parts-of-models.html", "Chapter 15 Testing parts of models 15.1 ANOVA reports 15.2 Non-Parametric Statistics", " Chapter 15 Testing parts of models require(mosaic) # mosaic operators and data used in this section The basic software for hypothesis testing on parts of models involves the familiar lm() and summary() operators for generating the regression report and the anova() operator for generating an ANOVA report on a model. 15.1 ANOVA reports The anova operator takes a model as an argument and produces the term-by term ANOVA report. To illustrate, consider this model of wages from the Current Population Survey data. Cps &lt;- CPS85 # from mosaicData mod1 &lt;- lm( wage ~ married + age + educ, data = Cps) anova(mod1) ## Analysis of Variance Table ## ## Response: wage ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## married 1 142.4 142.40 6.7404 0.009687 ** ## age 1 338.5 338.48 16.0215 7.156e-05 *** ## educ 1 2398.7 2398.72 113.5405 &lt; 2.2e-16 *** ## Residuals 530 11197.1 21.13 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note the small p-value on the married term: 0.0097. To change the order of the terms in the report, you can create a new model with the explanatory terms listed in a different order. For example, here’s the ANOVA on the same model, but with married last instead of first: mod2 &lt;- lm( wage ~ age + educ + married, data = Cps) anova(mod2) ## Analysis of Variance Table ## ## Response: wage ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 440.8 440.84 20.8668 6.13e-06 *** ## educ 1 2402.7 2402.75 113.7310 &lt; 2.2e-16 *** ## married 1 36.0 36.01 1.7046 0.1923 ## Residuals 530 11197.1 21.13 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now the p-value on married is large. This suggests that much of the variation in wage that is associated with married can also be accounted for by age and educ instead. 15.2 Non-Parametric Statistics Consider the model of world-record swimming times plotted on page 116. It shows pretty clearly the interaction between year and sex. It’s easy to confirm that this interaction term is statistically significant: Swim &lt;- SwimRecords # in mosaicData anova( lm( time ~ year * sex, data = Swim) ) ## Analysis of Variance Table ## ## Response: time ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## year 1 3578.6 3578.6 324.738 &lt; 2.2e-16 *** ## sex 1 1484.2 1484.2 134.688 &lt; 2.2e-16 *** ## year:sex 1 296.7 296.7 26.922 2.826e-06 *** ## Residuals 58 639.2 11.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The p-value on the interaction term is very small: \\(2.8 \\times10^{-6}\\). To check whether this result might be influenced by the shape of the distribution of the time or year data, you can conduct a non-parametric test. Simply take the rank of each quantitative variable: mod &lt;- lm( rank(time) ~ rank(year) * sex, data = Swim) anova(mod) ## Analysis of Variance Table ## ## Response: rank(time) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## rank(year) 1 14320.5 14320.5 3755.7711 &lt;2e-16 *** ## sex 1 5313.0 5313.0 1393.4135 &lt;2e-16 *** ## rank(year):sex 1 0.9 0.9 0.2298 0.6335 ## Residuals 58 221.1 3.8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 With the rank-transformed data, the p-value on the interaction term is much larger: no evidence for an interaction between year and sex. You can see this directly in a plot of the data after rank-transforming time: xyplot( rank(time) ~ year, groups = sex, data = Swim) The rank-transformed data suggest that women’s records are improving in about the same way as men’s. That is, new records are set by women at a rate similar to the rate at which men set them. "],
["models-of-yesno-variables.html", "Chapter 16 Models of Yes/No Variables 16.1 Fitting Logistic Models 16.2 Fitted Model Values 16.3 Which Level is “Yes”? 16.4 Analysis of Deviance", " Chapter 16 Models of Yes/No Variables Fitting logistic models uses many of the same ideas as in linear models. library(mosaic) 16.1 Fitting Logistic Models The glm operator fits logistic models. (It also fits other kinds of models, but that’s another story.) glm takes model design and data arguments that are identical to their counterparts in lm. Here’s an example using the smoking/mortality data in Whickham: mod = glm( outcome ~ age + smoker, data=Whickham, family=&quot;binomial&quot;) The last argument, family=&quot;binomial&quot;, simply specifies to glm() that the logistic transformation should be used. glm() is short for Generalized Linear Modeling, a broad label that covers logistic regression as well as other types of models involving links and transformations. The regression report is produced with the summary operator, which recognizes that the model was fit logistically and does the right thing: summary(mod) ## ## Call: ## glm(formula = outcome ~ age + smoker, family = &quot;binomial&quot;, data = Whickham) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9581 -0.5458 -0.2228 0.4381 3.2795 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.599221 0.441231 -17.223 &lt;2e-16 *** ## age 0.123683 0.007177 17.233 &lt;2e-16 *** ## smokerYes 0.204699 0.168422 1.215 0.224 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1560.32 on 1313 degrees of freedom ## Residual deviance: 945.02 on 1311 degrees of freedom ## AIC: 951.02 ## ## Number of Fisher Scoring iterations: 6 Keep in mind that the coefficients refer to the intermediate model values \\(Y\\). The probability \\(P\\) will be \\(e^Y / (1 + e^Y )\\). 16.2 Fitted Model Values Logistic regression involves two different kinds of fitted values: the intermediate “link” value \\(Y\\) and the probability \\(P\\). The fitted operator returns the probabilities: probs = fitted(mod) There is one fitted probability value for each case. The link values can be gotten via the predict operator linkvals = predict(mod, type=&quot;link&quot;) head(linkvals) ## 1 2 3 4 5 6 ## -4.5498092 -5.1682251 1.3869832 0.6875514 0.3165019 -2.6945616 Notice that the link values are not necessarily between zero and one. The predict operator can also be used to calculate the probability values. response_vals = predict(mod, type=&quot;response&quot;) head(response_vals) ## 1 2 3 4 5 6 ## 0.010458680 0.005662422 0.800110184 0.665421999 0.578471493 0.063295027 This is particularly useful when you want to use predict to find the model values for inputs other than that original data frame used for fitting. 16.3 Which Level is “Yes”? In fitting a logistic model, it’s crucial that the response variable be categorical, with two levels. It happens that in the Whickham data, the outcome variable fits the bill: the levels are Alive and Dead. The glm software will automatically recode the response variable as 0/1. The question is, which level gets mapped to 1? In some sense, it makes no difference since there are only two levels. But if you’re talking about the probability of dying, it’s nice not to mistake that for the probability of staying alive. So make sure that you know which level in the response variable corresponds to 1: it’s the second level. Here is an easy way to make sure which level has been coded as “Yes”. First, fit the all-cases-the-same model, outcome ~ 1. The fitted model value \\(P\\) from this model will be the proportion of cases for which the outcome was “Yes.” mod2 = glm( outcome ~ 1, data = Whickham, family = &quot;binomial&quot;) fitted(mod2) So, 28% of the cases were “Yes.” But which of the two levels is “Yes?” Find out just by looking at the proportion of each level: tally( ~ outcome, data = Whickham, format = &quot;proportion&quot;) ## ## Alive Dead ## 0.7191781 0.2808219 If you want to dictate which of the two levels is going to be encoded as 1, you can use a comparison operation to do so: mod3 = glm( outcome == &quot;Alive&quot; ~ 1, data=Whickham, family=&quot;binomial&quot;) mod3 In this model, “Yes” means Alive. 16.4 Analysis of Deviance The same basic logic used in analysis of variance applies to logistic regression, although the quantity being broken down into parts is not the sum of squares of the residuals but, rather, the deviance. The anova software will take apart a logistic model, term by term, using the order specified in the model. anova(mod, test=&quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: outcome ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 1313 1560.32 ## age 1 613.81 1312 946.51 &lt;2e-16 *** ## smoker 1 1.49 1311 945.02 0.2228 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice the second argument, test=&quot;Chisq&quot;, which instructs anova to calculate a p-value for each term. This involves a slightly different test than the F test used in linear-model ANOVA The format of the ANOVA table for logistic regression is somewhat different from that used in linear models, but the concepts of degrees of freedom and partitioning still apply. The basic idea is to ask whether the reduction in deviance accomplished with the model terms is greater than what would be expected if random terms were used instead. "],
["causation.html", "Chapter 17 Causation", " Chapter 17 Causation "],
["experiment.html", "Chapter 18 Experiment", " Chapter 18 Experiment "]
]
